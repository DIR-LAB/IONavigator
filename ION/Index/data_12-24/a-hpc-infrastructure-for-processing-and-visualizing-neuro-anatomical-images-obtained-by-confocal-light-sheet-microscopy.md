# A HPC Infrastructure for Processing and Visualizing Neuro-anatomical Images Obtained by Confocal Light Sheet Microscopy

Alessandro Bria∗ , Giulio Iannello† , Paolo Soda† , Hanchuan Peng‡ , Giovanni Erbacci§ , Giuseppe Fiameni§ , Giacomo Mariani§ , Roberto Mucci§ , Marco Rorro§ , Francesco Pavone¶ , Ludovico Silvestri¶ , Paolo Frasconik and Roberto Cortinik ∗ *Department of Electrical and Information Engineering University of Cassino and Lazio Meridionale, Cassino (FR), Italy* † *Integrated Research Center, University Campus Bio-Medico of Rome, Italy* ‡ *Allen Institute for Brain Science, Seattle, WA, USA Janelia Farm Research Campus, Howard Hughes, Medical Institute, Ashburn, VA, USA* § *SuperComputing Applications and Innovation Department Cineca - Interuniversity Consortium, Casalecchio di Reno (BO), Italy* ¶ *European Laboratory for Non-Linear Spectroscopy (LENS), University of Florence, Italy* k *Information Engineering Department, University of Florence, Italy*

*Abstract*—Scientific problems dealing with the processing of large amounts of data require efforts in the integration of proper services and applications to facilitate the research activity, interacting with high performance computing resources. Easier access to these resources have a profound impact on research in neuroscience, leading to advances in the management and processing of neuro-anatomical images. An ever increasing amount of data are constantly collected with a consequent demand of top-class computational resources to process them. In this paper, a HPC infrastructure for the management and the processing of neuro-anatomical images is presented, introducing the effort made to optimize and integrate specific applications in order to fully exploit the available resources.

*Keywords*—HPC, Data, Neuroscience, Visualisation, Human Brain Project, Confocal Microscopy

# I. INTRODUCTION

Contemporary science has to tackle with an ever increasing amount of data, and biological sciences make no exception. Indeed, the automatization of imaging techniques such as optical and electron microscopy is making possible to collect larger and larger image datasets, which nowadays easily exceed one TeraByte each. In parallel to technical developments improving the speed and throughput of data generation, new computational paradigms are needed to cope with these large datasets, in order to discover new insights. The research field in computational analysis of biological images has been recently named Bioimage Informatics [1], and several tools are now available to deal with important problems in bioimage analysis. However, even state-of-the-art tools cannot generally cope with images of dimensions exceeding tens of GigaBytes so novel tools are needed, especially designed to operate on TeraByte-sized datasets. A further issue that arises when dealing with very large images, is that all the processing pipelines from image acquisition to storage and retrieval have to be carefully designed and implemented to maintain resource requirements and response times within acceptable limits. In particular high performance computing techniques have to be extensively employed to meet application requirements.

To respond to the increasing complexity of manipulation and processing these very large datasets, an IT infrastructure has been set up to provide data management and high performance computing capabilities. Data handled are mouse brain images obtained using CLSM (Confocal Light Sheet Microscopy) [2], a confocal ultra-microscopy technique in which selectively labelled neurons are imaged by light-sheet based microscopy [3] [4] with micron-scale resolution. Data obtained from an experiment conducted on a mouse brain (1 cubic cm at micrometric resolution) might be of a range of 1 TeraByte, or more. Specific applications have been implemented in a toolkit at disposal of the scientists in order to perform: 1) fully automated 3D Stitching capability starting from acquired raw data and 2) semi-automatic extraction of some morphological characteristics (e.g. neurons localization) [2] [5] and 3) interactive visualization and annotation of images. Data and software tools, as well as elaboration algorithms are made available through a dedicated storage and computational infrastructure operated by Cineca [6], the largest Italian computing center. Data sets originated from the CLSM at the European Laboratory of Nonlinear Spectroscopy LENS [7] are transferred to Cineca using high performance protocol (i.e. GridFTP) and successively stored using the iRODS Data Grid technology [8]. The remote visualization of data sets is provided through a specific service called Remote Connection Manager developed by Cineca to manage remote connections towards the visualization facility.

Cineca and LENS are partners of the Human Brain Project, the EU Flagship project started in October 2013 and aiming to build a new information computing infrastructure for neuroscience and for brain-related research in medicine and computing, catalysing a global collaborative effort to understand the human brain and its diseases. Cineca represents one of the four HPC infrastructures supporting of the HBP initiative. In particular Cineca will implement the HBP Massive Data Analytics Supercomputer Infrastructure where supercomputers will be integrated with a Data Facility of around 5 Petabytes for on-line disk storage repository and more 10 Petabytes for long term data preservation, providing efficient data life-cycle management of structured and unstructured data generated by HBP scientists. The objective is to provide an efficient Data Centric HPC facility to store, manage and process the data images produced by the HBP and, in particular, by the LENS, related to the neuro-anatomy of the entire mouse brain and possibly larger brain samples up to the primate scale brain. The infrastructure presented in this paper represents a contribute to the seminal implementation of the HBP Massive Data Analytics Infrastructure.

## II. MOTIVATIONS AND REQUIREMENTS

#### *A. Image characteristics*

The CLSM apparatus acquires several parallel image stacks, in order to cover the entire volume occupied by the mouse brain. A single image stack consists of 103 to 104 bidimensional images, each one containing 2048×2048 pixels. Typical voxel size is 0.35×0.35×1 µm3 ; images are 16-bit grayscale. Each raw image stack is saved as a single .dwg file, the file format provided by the camera drivers, and can be subsequently converted to a standard image file format, such as .tiff. Raw stacks are saved without compression, and the largest ones might occupy a disk space of the order of ∼ 80 GigaBytes. To cover the volume of an entire mouse brain, 100 to 400 stacks of various length might be needed. Assuming the mouse brain volume to be about 0.5 cm3 and the typical voxel size as reported above, the complete raw dataset occupies about 4 TeraVoxels, i.e. 8 TeraBytes of disk space (since the voxel gray depth is 16 bit). Image stacks are acquired at a maximum speed of 50 frame per second, corresponding to a data stream of 400 MB/s.

## *B. Image processing pipeline*

CLSM images processing consists of several steps that can be organized as shown in Figure 1.

After the microscope controlling software has generated the uncompressed raw images corresponding to overlapping tiles that cover all the volume of interest, images have to be

![](_page_1_Figure_8.png)

Figure 1. Steps composing the image processing pipeline

uploaded to the data repository at Cineca site for subsequent processing. To reduce the communication overhead and space requirement, some lossless compression is performed before transmission.

After raw data are stored in the repository, stitching has to be performed before any other step in order to have a continuous, non redundant image of the volume of interest. This step requires reading at least two times all image data and write it back once. It is therefore demanding with respect to response times. For this reason parallelizing both code and I/O is needed to deliver an effective service. In Section IV-A we will give more details on this issue.

Stitched data should be stored again in the repository in compressed form to reduce space requirements. A special multiresolution image representation is generated by the stitching process to enable efficient access to the visual content in subsequent steps. Also meta-data enabling information retrieval functions have to be generated at this time. Meta-data must completely describe image contents and the acquisition process and are automatically generated from data attached to images by the acquisition software.

The other steps shown in Figure 1 correspond to possible manipulations of images to both improve their quality and extract useful biological information. They are very heterogeneous in nature spanning from signal processing operations, like denoising and deconvolution, to content-based operations, like atlas registration or cell counting, to user-oriented operations, like visualization and annotation. A common characteristics of all of them is however the need of high performance computing to maintain user activity within acceptable limits. Indeed, while content based operations may generate very high computational costs per unit data, performance of other steps, like denoising and visualization, is basically I/O bounded, requiring high performance I/O architectures.

## *C. User interaction models*

Although efforts will be put in making the new software tools to be developed as much user friendly as possible, most of them will remain necessarily experimental in nature since advances in imaging techniques push their continuous development. This requires a flexible model for user interaction that makes it possible for biologists using tools at least for testing even in their early development stages.

Indeed, on the one hand, supercomputing facilities are needed to perform operations on whole large images, suggesting a web-based or a client/server interaction model, in which source data remain at the remote site, where processing is carried out using high performance platforms. On the other hand, when a new experimental tool is in its initial stages of development, its functionalities are not completely defined and it needs several test-and-modify iterations before reaching a stable configuration. In this case neither the tool can be systematically applied to very large data sets, nor it supports an easy to use remote interface, suggesting an interaction model where a relatively limited amount of test data are transferred from the repository to the user site, where data is processed locally by the experimental tool.

For these reasons we decided to support two different strategies for information retrieval by the user and subsequent processing. The first strategy allows for launching the requested operation directly on remotely stored data, using the supercomputing facilities available for processing at the Cineca site. Conversely, the second strategy allows the user to select the data he/she want to manipulate, transfer them in the proper format at user site, and then perform processing with locally installed applications.

## III. THE CINECA ARCHITECTURE

This section describes the infrastructure at disposal of LENS researchers for the transfer, the preservation, the processing and the visualisation of data sets. The core concept of our implementation is to enable researchers and managers to deposit and process data acquired from microscopy experiments in an efficient way hiding them the complexity of the underneath technologies. This section describes the capabilities needed to support this core concept, and how they map to technologies selected to build the infrastructure.

The infrastructure provides capabilities for storing, sharing, processing and visualizing neuro-anatomical data amongst researchers of the LENS organization, and potentially of other research communities, such as the INCF infrastructure [9] or the Human Brain Project [10]. Some of the use cases that our services support include:

- (a) Providing capabilities so that LENS scientists can easily transfer and deposit raw data sets onto Cineca resources for being preserved on the long-term. The replication of data is performed through robust mechanisms to ensure the transfer of large volume of data complete successfully and is tolerant to network and system failures. In addition, integrity of deposited data is ensured through periodic checks on objects checksum. Unused or less frequently accessed data are eventually migrated to tape after a certain period of time, typically few months. If a user does reuse a file which is on tape, it is automatically moved back to disk storage. The advantage of this solution is to maintain on online storage resources only those data sets which are accessed frequently saving space. The transmission of data is performed using the GridFTP protocol [11] while the transfers are managed through the Globus OnLine service [12]. The combination of these two last technologies guarantees the exploitation of a large fraction of the available network bandwidth and the automatic resume of interrupted transfers.
- (b) Providing tools and resources for the processing of transferred data. The infrastructure includes the Cineca PLX Linux cluster with 274 IBM iDataPlex nodes, each equipped with 2 six-cores Intel Westmere at 2.4 GHz (3288 cores in total) and 48 GB of RAM per node. PLX is accelerated with 548 GPUs TESLA M2070 (2 per node) and has a peak performance of 300 TFlop/s.
- (c) Providing capabilities so that data can be shared with other researchers outside the LENS organization and access to data is regulated through a fine-grained authorization mechanism.
- (d) Providing capabilities so that researchers can browse and access datasets remotely regardless of their physical location. In the future, for some types of files, browsing could also include retrieving single parts of data. Access to datasets includes the remote visualization of image files.
- (e) Providing a mechanism for annotating data with basic meta-data, and allowing users to search through the entire repository using meta-data terms.

Figure 2 presents the architecture of the platform. The platform is implemented using various technologies. The required capabilities for the data fabric are provided through the use of the iRODS rule-oriented data system software. The iRODS software utilizes a back-end MySQL database to provide a virtual namespace across an arbitrary set of resources, and includes features to automate replication and resource selection for both

![](_page_3_Figure_0.png)

Figure 2. Architecture

storage and retrieval. It also allows for multiple administrative domains, or zones, to be federated into a single system with a single set of credentials used to authenticate and perform access control operations across all included resources in the various zones. This allows the LENS researchers to focus on simple data ingestion and retrieval operations, while the storage layer can be separately administered and data location and replication features can be managed transparently without requiring user intervention into the configuration. Arbitrary meta-data can also be stored along with datasets, and they can be searched for based on this meta-data. The iRODS data transport layer supports common high-performance network transfer features including large window sizes and threaded transfers, allowing the underlying data management system to take advantage of high-speed networks to provide the necessary performance in moving TeraBytes of data between the two sites systems. The existing features were enhanced to support the GridFTP protocol which is the *de facto* standard for high-performance transfers in the HPC field. This enhancement permits to exploit the whole network bandwidth available between Cineca and LENS, implemented through a dedicated 10Gb/s network path. The need for a dedicated network emerged after the upgrade of the microscopy facility at LENS (October 2013) and the consequent production of larger images taken at a higher level of resolution. Currently, the data fabric stores more than 63 million of objects organized in a directories tree and reflecting the original organization of the image stacks. This large amount of stored objects has rapidly requested the definition of a data management strategy to optimize the access performance. The most relevant improvement was the aggregation of the files belonging to the branch in larger archive thus significantly reducing the load on the file system as less UNIX inodes need to be created. Finally, data sets ingested into the data fabric can be easily stated on the PLX working area for being further processed or remotely visualized through a specific service developed at this purpose. The remote visualization service is provided

through a Remote Connection Manager [13], a python cross platform application that simplifies and automates the steps needed for setting up a VirtualGL/VNC [14] connection to nodes equipped with graphic cards.

#### IV. IMPLEMENTATION

## *A. Stitching*

The first tool we implemented to deal with terabyte-sized 3D images is a fully automated 3D stitching tool designed for the very large images acquired by CLSM microscope. The tool has been named *Terastitcher* and it makes possible the accurate reconstruction of images of virtually any dimension even on workstations with limited resources in a reasonable time. This result is obtained by exploiting the characteristics of CLSM acquisitions, in particular the assumption that relative tile positions are known with a very good approximation.

The stitching strategy implemented by Terastitcher aims at minimizing I/O operations. Indeed, since the final representation of the whole 3D image cannot be produced if the precise relative positions of tiles have not been determined, alignment of all tiles has to be performed before starting the actual stitching. Since the entire dataset cannot be kept in memory, all data have to be read at least twice, and written back to mass storage in its final representation once.

Another peculiar feature of Terastitcher is that it does not use 3D alignment algorithms, but rather it relies on multiple 2D alignments among the overlapping regions of contiguous tiles, leading to a remarkable reduction in computational workload. Interested readers may find more details in [5].

Figure 3 shows the steps composing the stitching pipeline. The most time consuming steps are the *Pairwise tasks displacement computation* and the *Merging and saving tiles into a multiresolution representation* step. In the first step the whole volume is processed one layer at a time, each composed of nslices slices at most, typically in the range [100, 200]. In the second step only a small group of slices for each tile are read at a time, merged and finally saved back to the file system.

Table I shows the time spent in these steps for computation (Tdispl.comp./Tmerging) and I/O (T I/O displ.comp./TI/O merging) in the case of a test dataset composed by 5 × 6 stacks of 1726 images (512 × 512 pixel). Each image is 262.5 KB for a total amount of about 13 GB of disk space. We used nslices = 100 as Terastitcher parameter. The first five resolutions are produced, from full resolution to one sixteenth, for a total amount of 3294 compressed images and almost 2 GB of disk space. Tests are carried out on a node of the Cineca PLX cluster. The reported elapsed times refer to the best run out of three executions. In fact, the underlying GPFS parallel file system [15] is shared by cluster users, making execution time vary on different executions, depending on the network traffic. Although modest in size, the test is anyway meaningful since execution times are linearly dependent on the dataset size. These results, as well as standard profiling tools, confirm that the execution time is I/O bound. Giving these preliminary assumptions, the core parts of the Terastitcher were improved through the multithread parallelisation of the code with OpenMP [16], a *de facto* standard for parallel programming on shared memory systems.

The main module of the TeraStitcher software are organized in three layers with growing abstraction level as shown in Figure 4. At the lowest level, the *IOManager* module contains the I/O methods. The middle layer contains the *VolumeManager* module responsible to model data organization and to access data through functionalities provided by the lower layer. At the top layer, the *Stitcher* module implements the stitching pipeline. In the *Pairwise stacks displacement computation* step, the loop in the *IOManager*, where slices are read in block of nslices at a time, was made parallel through the #pragma omp parallel for pragma that open a parallel region and distribute the iterations of the loop across threads. Furthermore we declared some variables to be private to each thread and peeled off the first iteration of the loop where the memory for the whole stack is allocated. In the *Merging and saving tiles into a multiresolution representation* step, a parallel region at the higher level is opened, both because just few images are read, then stitched and written back layer by layer, and because this step is embarrassing parallel by nature. Thus the parallel region is opened before calling this step and then the loop over the different image layers is distributed across threads. As in the previously discussed step, some variables are taken private and the first iteration of the loop, where the hierarchical directories structure is created, is peeled off.

Figure 5 shows the speedup (the ratio of serial execution time to parallel execution time) of the parts interested by the parallelization and the total speedup. It is worth noting that a good speedup factor is also obtained for the I/O parts, and that it is better for the merging step, where the parallel region is wider. We expect that widening the parallel region in the displacement computation step, so to include the computational part, can further improve the overall performance as well as that of the I/O part. These results are also obtained with minimal changes to the code.

For the sake of completeness we conclude this paragraph describing the other changes we made to the code to take care of some minor features like timing and error handling.

![](_page_4_Figure_8.png)

1009 1283 668 1728 4688

Computational and I/O time (seconds) of the most relevant stitching phases

![](_page_5_Figure_0.png)

Figure 4. The software architecture of TeraStitcher

![](_page_5_Figure_2.png)

The static member used to store elapsed time was declared to be threadprivate and the OpenMP omp_get_wtime function was used to measure time. Finally, while waiting for the implementation of the new cancel feature of OpenMP 4.0, in order to handle errors in parallel regions, we exploited the new C++11 feature for copying and rethrowing exceptions. We defined a new exception class made of a omp_lock_t variable, a std::exception_prt, a method to capture exception if the lock is unset, and one to rethrow exception. We used it, inside parallel regions, to capture the first exception and then rethrow it beyond the end of the parallel region.

#### *B. Multi-resolution representation of large 3D images*

As briefly mentioned in Section II-B, a multi-resolution representation has been introduced to store stitched images in a form suited for further efficient access. This representation consists of multiple resolutions of the same 3D image, organized in a hierarchical structure where at the bottom there is the highest resolution image (the one obtained by stitching the raw data) and every higher level is obtained by merging eight contiguous voxels into one voxel. This way the image dimension reduces at each level of the hierarchy by a factor of 8. Note that this representation can be effectively exploited for image navigation as better explained in the next section, but it turns out to be advantageous also in other operations that manipulate images. For instance if multiple very large images have to be co-registered, performing co-registrations progressively at different resolutions may lead to very precise co-registrations at lower computational costs. Also in image analysis operations a multi-resolution representation can reduce computational costs, when, for instance, the analysis focuses on objects that may have very different size in an image corresponding to a large volume (e.g. vessels). Indeed, larger objects can be processed at a lower resolution, which requires less I/O and processing workload.

# *C. Integration with Vaa3D and other currently available functionalities*

If the raw data to be stitched do not exceed a few hundreds of GB, the performance of the Terastitcher on a high end workstation is still acceptable. To offer an easy to use stitching facility in cases in which stitching has to be performed locally by the user, we have integrated the sequential version of the Terastitcher into the Vaa3D software [17] as a plug-in [18]. The choice of Vaa3D was motivated by its versatility and popularity, as well as by its orientation from the very beginning to efficient image processing. Indeed, while the original core of the package could deal with datasets up to tens of GB, after the first step towards terabyte-sized images represented by the integration of the Terastitcher plug-in, we have continued to extend Vaa3D capabilities, developing other plugins that use the multi-resolution representation described in the previous section. In particular we have developed a new plug-in named Vaa3D-TeraFly, which extends the 3D visualization and annotation capabilities of Vaa3D enabling navigation into images of virtually any size in a Google Earth-like fashion. Currently, Vaa3D-TeraFly provides many functions useful for inspecting and proofreading very large images in a way that would not be possible otherwise. Its effectiveness, however, highly relies on rendering capabilities on user screen, making it suited for a fundamentally local interaction model, where visual data are transferred at user site before starting a work session.

Since Vaa3D-TeraFly capabilities are not limited to visualizing images stitched with Terastitcher, a third Vaa3D plugin, named Teraconverter, has been developed to generate the multi-resolution representation required by the Vaa3D-TeraFly from different 3D image formats.

#### *D. Cell-counting: an example of image analysis*

Large-scale anatomical investigations aimed at obtaining detailed maps of the brain architecture are becoming crucially important in modern neuroscience [19]. Current methods are unable to obtain accurate fully automated analyses of large 3D brain images [20]. In this activity we have developed large-scale methods capable of analysing whole brain images and identify cell nuclei in a fully automated fashion. Our method (see [21] for details) combines three core algorithmic ideas, briefly summarized below: (1) mean shift clustering to identify nuclei centers, (2) supervised semantic deconvolution by means of deep networks for image enhancement, and (3) manifold learning for filtering false positive detections. This algorithmic pipeline has been tested on a micron resolution image of a transgenic mouse cerebellum with enhanced GFP labelling and acquired by confocal light-sheet microscopy. Image size in this type of applications typically exceed 100 GVoxels and clearly require careful parallelization strategies in order to be practical. Our approach consists of dividing a large image into several smaller substacks on which the analysis can be carried out in parallel. This requires, however, to share (very large) images across several computers and parallel I/O may represent the bottleneck of the whole procedure. However, since the application requires that a very large input image has to be shared among multiple processing units, to achieve true scalability of the parallel implementation even I/O operations have to be parallelized.

*1) Unsupervised cell identification:* Mean shift [22] is a non parametric mode-seeking algorithm that clusters voxels into groups that ideally correspond to cell nuclei. In its classic formulation the algorithm starts from all available data points or from a subset of seeds, place a kernel around each of them, and *shifts* them towards the mean value computed as the kernel-weighted average of the data. Its running time is dominated by the execution of ball queries to a KD-tree [23] for determining the set of points covered by the kernel. Parallelization of mean shift in this context can be achieved either at the seed level (seeds are updated independently) or at the substack level. The second approach does not require shared memory and simply allocates a clustering job to the next available CPU core. Different threads, however, need parallel access to storage for fetching the relevant image portion. Our current parallel implementation is based on substacking. It relies on the Python SciPy implementation of KD-trees and the Slurm job scheduler and is capable of processing between 30 and 100 GVoxels per hour (depending on the algorithm parameters) on a small cluster of two servers each equipped with two eight-core Xeon E5-2665 boards (for a total of 64 parallel jobs, taking advantage of hyper-threading).

*2) Supervised semantic deconvolution:* This technique is based on a non-linear filter (implemented by a deep neural network [24]) trained to purge from the original image all uninteresting regions (such as dendritic arborizations) and equalize the visibility of cell nuclei. The network takes as input a cubic patch of (2s + 1)3 inputs and predicts a corresponding cubic patch of the same size. The volume is processed in a convolution-like fashion skipping d voxels along each dimension when moving the patch along the volume. The overall running time is O(n 3 s 3/d3 ). Our deep network has about 1.6 millions of parameters for s = 6 and using two layers of 500 and 200 hidden units, respectively. For d = 4, processing a 120 GVoxel image requires approximately 3 PFlops. Our implementation takes advantage of the Python Theano library [25] which supports CPU and GPU processing in a transparent fashion. GPUs are currently the best available option for running neural networks. The availability of 548 Tesla M2070 makes the Cineca platform extremely appealing for this type of applications. Alternatively, when using CPUs, the use of efficient linear algebra libraries (such as Intel MKL) is crucial for exploiting multi-core architectures. In both cases, processing one sub-stack on each computing node and allowing several cores (or a GPU) within the same node to process the substack using parallelism at the numerical level appears to be the best strategy.

*3) Manifold filtering:* The final step in the cell identification pipeline takes advantage of specific anatomical background knowledge. In several brain regions, cells are layed out in folia that are naturally modeled as manifolds. Since off-manifold detections are almost invariably false positives, an effective filter may be designed by estimating the distance of each predicted center from the underlying manifold. Our approach combines the Isomap algorithm [26]) and locally weighted regression [27] to obtain such an estimate. Isomap is not easily parallelized as it requires to compute the eigendecomposition of the shortest-path distance matrix in a nearest-neighbors graph. We therefore partition the predicted surface into smaller regions, an approach known as *chartification* in computer graphics [28]. Isomap and locally weighted regression can then be run in parallel on separate charts.

#### V. CONCLUSIONS

Increasingly amounts of experiments are conducted in the field of the neuroscience producing large volume of data which need to be properly managed and processed to extract new scientific insights. The traditional systems struggle to handle these increasing amounts of data. An adequate set of services and applications needs to be implemented and integrated together to respond to researches demand of storage and computational resources. This paper has described the design and the implementation of an integrated HPC infrastructure for gathering, processing and visualising neuro-anatomical images produced by CLSM at LENS. Details have been given about the core components of the infrastructure ranging from the data fabric, implemented through the adoption of the iRODS technology, the applications and the tools implemented for the analysis and the final visualization of the data sets. Improvements made to optimize the applications performance in using the HPC system have been also addressed. This novel approach allows neuro scientists to track, manage and analyse data easily, as well as share their results with other colleagues, such as those of the INCF federation of the Human Brain Project.

#### REFERENCES

- [1] H. Peng, "Bioimage informatics: a new area of engineering biology," *Bioinformatics*, vol. 24, no. 17, pp. 1827–1836, 2008.
- [2] L. Silvestri, A. Bria, L. Sacconi, G. Iannello, and F. S. Pavone, "Confocal light sheet microscopy: micron-scale neuroanatomy of the entire mouse brain," *Optics Express*, vol. 20, no. 18, pp. 20 582–20 598, 2012.

- [3] H.-U. Dodt, U. Leischner, A. Schierloh, N. Jhrling, C. P. Mauch, K. Deininger, J. M. Deussing, M. Eder, W. Zieglgnsberger, and K. Becker, "Ultramicroscopy: three-dimensional visualization of neuronal networks in the whole mouse brain," *Nature Methods*, vol. 4, no. 4, pp. 331–336, 2007.
- [4] P. J. Keller and H.-U. Dodt, "Light sheet microscopy of living or cleared specimens," *Current Opinion in Neurobiology*, vol. 22, no. 1, pp. 138 – 143, 2012.
- [5] A. Bria and G. Iannello, "TeraStitcher A tool for fast automatic 3Dstitching of teravoxel-sized microscopy images," *BMC Bioinformatics*, vol. 13:316, 2012.
- [6] Cineca Interuniversitary Consortium. Department of SuperComputing Applications and Innovation. Bologna, Italy. http://www.hpc.cineca.it.
- [7] European Laboratory for Non-Linear Spectroscopy. University of Florence.
- http://www.lens.unifi.it/. [8] iRODS (Integrated Rule-Oriented Data System). https://www.irods.org/.
- [9] International Neuroinformatics Coordination Facility. http://www.incf.org/.
- [10] The Human Brain Project. https://www.humanbrainproject.eu/.
- [11] Grid File Transfer Protocol. http://toolkit.globus.org/toolkit/docs/latest-stable/gridftp/.
- [12] Globus OnLine Service.
- https://www.globus.org/tags/globus-online. [13] Cineca - Remote Connection Manager.
- http://www.hpc.Cineca.it/services/remote-visualisation. [14] VirtualGL.
- http://sourceforge.net/projects/virtualgl/. [15] IBM General Parallel File System (GPFS). http://www.ibm.com/systems/software/gpfs/.
- [16] OpenMP Architecture Review Board. (2011) OpenMP application program interface version 3.1. http://www.openmp.org.
- [17] H. Peng, Z. Ruan, F. Long, J. H. Simpson, and E. Myers, "V3D enables real-time 3D visualization and quantitative analysis of largescale biological image data sets," *Nature Biotechnology*, vol. 28, no. 4, pp. 348–353, 2010.

- [18] H. Peng, A. Bria, Z. Zhou, G. Iannello, and F. Long, "Extensible visualization and analysis for multidimensional images using Vaa3D," *Nature Protocols*, vol. 9, no. 1, pp. 193–208, 2014.
- [19] H. Peng, B. Roysam, and G. Ascoli, "Automated image computing reshapes computational neuroscience," *BMC Bioinformatics*, vol. 14:293, 2013.
- [20] H. Peng, F. Long, T. Zhao, and E. Myers, "Proof-editing is the Bottleneck Of 3D Neuron Reconstruction: The Problem and Solutions," *Neuroinformatics*, vol. 9, no. 2-3, pp. 103–105, 2011.
- [21] P. Frasconi, L. Silvestri, P. Soda, R. Cortini, F. S. Pavone, and G. Iannello, "Large-Scale Automated Identification of Mouse Brain Cells in Confocal Light Sheet Microscopy Images," submitted.
- [22] D. Comaniciu and P. Meer, "Mean shift: a robust approach toward feature space analysis," *Pattern Analysis and Machine Intelligence, IEEE Transactions on*, vol. 24, no. 5, pp. 603–619, 2002.
- [23] J. L. Bentley, "Multidimensional Binary Search Trees Used for Associative Searching," *Commun. ACM*, vol. 18, no. 9, pp. 509–517, 1975.
- [24] G. E. Hinton, S. Osindero, and Y.-W. Teh, "A Fast Learning Algorithm for Deep Belief Nets," *Neural Compututation*, vol. 18, no. 7, pp. 1527– 1554, 2006.
- [25] Bergstra, James and Breuleux, Olivier and Bastien, Fred´ eric and Lam- ´ blin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Bengio, Yoshua, "Theano: a CPU and GPU math expression compiler," in *Proceedings of the Python for Scientific Computing Conference (SciPy)*, 2010.
- [26] J. B. Tenenbaum, V. d. Silva, and J. C. Langford, "A Global Geometric Framework for Nonlinear Dimensionality Reduction," *Science*, vol. 290, no. 5500, pp. 2319–2323, 2000.
- [27] W. S. Cleveland, "Robust Locally Weighted Regression and Smoothing Scatterplots," *Journal of the American Statistical Association*, vol. 74, no. 368, pp. 829–836, 1979.
- [28] K. Zhou, J. Synder, B. Guo, and H.-Y. Shum, "Iso-charts: Stretchdriven Mesh Parameterization Using Spectral Analysis," in *Proceedings of the 2004 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing*, 2004, pp. 45–54.

