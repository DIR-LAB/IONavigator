2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software 2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software 2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software 2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software 2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software 2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software 2014 IEEE International Conference on High Performance Computing and Communications (HPCC), 2014 IEEE 6th International Symposium on Cyberspace Safety and Security (CSS) and 2014 IEEE 11th International Conference on Embedded Software

and Systems (ICESS) and Systems (ICESS) and Systems (ICESS) and Systems (ICESS) and Systems (ICESS) and Systems (ICESS) and Systems (ICESS)

# Online Performance Analysis: An Event-based Workflow Design Towards Exascale

Michael Wagner, Tobias Hilbrich, and Holger Brunst Center for Information Services and High Performance Computing (ZIH) Technische Universitat Dresden ¨ 01062 Dresden, Germany Email:{michael.wagner2, tobias.hilbrich, holger.brunst}@tu-dresden.de

*Abstract*—Today, it is commonly accepted that speedup and efficiency are not granted automatically when developing or porting software for High Performance Computing (HPC) platforms. The reasons are manifold and actively investigated in the community. Software monitors are essential for these studies as they provide the raw performance data to be analyzed. We propose an online monitoring workflow for event-based performance analysis that takes into account the significant changes in system architecture towards the development of exascale supercomputers. Critical properties are: communication across large numbers of processing elements, limited I/O capabilities, and a decreasing memoryper-core ratio. We present a hierarchical data management and steering workflow that directly couples the application, measurement, and analysis processes, thus eliminating the need for extensive communication and buffering. The workflow is closely integrated with the native system communication API to enable best communication across processing elements. The memory issue is addressed with a new lossy hierarchical data compression technique for in-memory storage, intended for small, fixed-size buffers. Further, we abandon secondary storage to avoid potential I/O challenges. We demonstrate the feasibility of our design with a prototype implementation that features services for data collection, analysis, and runtime compression. Our evaluation extrapolates results obtained with the NAS Parallel Benchmarks at up to 2,048 processes to an exascale workflow.

# I. INTRODUCTION

Today, High Performance Computing (HPC) is typically implemented by a large number of processing elements working jointly on a computationally intensive problem. For the next milestone—exascale supercomputers, i. e., systems capable of O(1018) floating point operations per second—this approach is very likely to persist. While Moore's law is expected to remain valid, limitations in clock frequency, instruction level parallelism, and energy density force system designers from industry to further scale-up the number of processing elements. Aside from that, the development of supercomputing hardware is strongly influenced by economy driven developments in the off-the-shelf computing domain. Accelerators from graphic cards are a prominent example for this influence. The history of TOP500 [1] systems shows that not all system characteristics improve at the same speed. Critical properties are: main memory bandwidth and latency, the amount of memory per core, I/O capabilities, as well as energy consumption [2]. Consequently, the first supercomputer to break the exascale barrier is likely to be a rather specialized solution with lots of computational power but also many constraints that have a considerable impact on efficient software development.

Parallel software that scales to the exascale implies the identification, distribution, and synchronization of millions of subproblems that can be computed independently. The partial results need to be joined effectively and efficiently to the overall solution of any computationally intensive problem. Writing software for systems of this scale is challenging and involves hybrid and potentially new programming models, accelerated computing, and energy considerations. An iterative performance design, verification, and optimization process is likely to be deployed to eventually exploit the vast resources of an exascale computer efficiently. Software debuggers, performance profilers and analyzers, and correctness checkers will continue to play an important role in this process. All of them require a software monitor component that collects relevant information of the parallel program under study for online or later processing. Program monitoring has been extensively addressed in many different soft- and hardware contexts ranging from embedded controller analysis to full-scale supercomputer studies [3], [4], [5], [6], [7], [8], [9]. Yet, with the hardware limitations of next generation supercomputers on the horizon, new questions arise.

In this paper, we address the impact of a decreasing memory-per-core ratio, limited I/O capabilities, and increasing core numbers of exascale systems on program monitoring. These limitations support our key hypothesis that monitoring and analysis need to become a fully integrated online process on future exascale systems. Section II classifies relevant and motivating use cases for such an online process, most of which already extend tool applicability on current systems. The main section of this paper presents the design of an online software analysis monitor that targets exascale compute systems. Key considerations include low I/O system capabilities and low memory per core ratios. We meet the I/O challenge with a hierarchical all-in-memory workflow that avoids use of the I/O subsystems altogether. The memory challenge is addressed with a library for lossy performance event compression that enables data management in fixed sized buffers of any (small) size. Efficient encoding and event reduction strategies are the key properties of this library. We employ Tree Based Overlay Network (TBON) techniques and high bandwidth tool communication—both demonstrated their applicability with HPC use cases—to couple an instrumentation component with this library, as well as to drive a scalable event analysis for performance data visualization. Section V briefly outlines a proof-of-concept prototype that we implement for verification and measurement purposes. A detailed evaluation focuses on tool overheads and memory per core footprints of our

851 851 851 851 839 839 839 prototype (Section VI). Finally, we discuss the importance of our online event monitoring approach at exascale. The combined instrumentation, recording, and steering capabilities enable interactive user driven event analysis sessions. This enables an interactive event-based exploration of the living patient, i. e. the running application from an inside perspective. Interesting use cases such as partial time frame recording or analysis refinement within the same run without the need for a restart arise from this mode of inspection/operation.

# II. USE CASES

A key motivation of the online performance analysis workflow that we detail in the following sections are architectural challenges of prospective exascale systems. At the same time, such a workflow enables new use cases that are hard to achieve—or even impractical—with current event-based performance analysis workflows. Such use cases include:

- (A) Inspect a running application over a long period of time: Let us assume a running parallel application that we have started a while ago. We would like to check the load distribution of our active application as we have added an improved distribution algorithm. The load distribution can be considered as a rather coarse performance metric. A monitor should be able to keep track of this kind of information with almost no overhead and a fixed size memory buffer from the start. Whenever the buffers are full, the recorded data should be thinned out.
- (B) Show the progress for a short period of time with many details: This scenario addresses long running production runs to be checked from time to time. In between these checks the given application is executing at full speed. Potential instrumentation from the monitor is either completely removed or turned off in way that reduces perturbation to a negligible amount. Whenever the monitor is turned on, it records data for a short period of time, compresses it into the given buffer and forwards it via a communication tree to a client consumer, who displays it graphically.
- (C) Alter the performance metrics to be inspected during runtime: Our performance analysis workflow focuses on the living patient, i. e., running applications. It is straightforward to think of a scenario where altering the metrics of interest during runtime is desireable. This idea is motivated by the fact that not all available hardware and software metrics can be recorded at the same time. Let's assume that metric A indicates a certain bottleneck in the application that can be underlined with metric B, which has not been recorded yet. With an online approach it becomes very easy to switch to metric B without the need for a potentially expensive application restart.
- (D) Stop on pre-defined conditions and provide recent history: Conditional *break points* known from debuggers can also be beneficial for event-based monitoring. An online approach can pause whenever critical conditions in an application occur. It can then provide a detailed event history about the circumstances that lead to the critical condition.

- (E) Save an observation permanently for further study: Our approach completely avoids the need for secondary storage. Yet, whenever an interesting snapshot of eventdata has been studied with the proposed online user interface, it should be possible to store the information for later reference. The big difference to a post-mortem approach is that this can happen on demand for multiple phases while the application continues to be executed. I/O requirements are very limited for this approach as the data only represents an aggregated snapshot of an application's phase.
- (F) Enable (third party) analysis modules to be hotplugged: A single tool provider cannot address all problems and questions. The proposed reduction and steering infrastructure needs well defined, stable, and open interfaces that allow third-party providers to hook into the measurement process at multiple stages that provide different levels of abstraction. Ideally, such plugins are hot-pluggable. This use-case also suggests that the observation of complete programming paradigms like MPI, OpenMP, or GPU acceleration can be turned on and off dynamically during runtime.

# III. RELATED WORK

Approaches towards online performance analysis include MALP [10], Paradyn [8], Periscope [6], Scalasca [11], SSB [4], TAUoverMRNet [9], and TAU-VNG [5]. The profiling approach MALP uses the capabilities of MPI [12] to connect monitoring components on multiple MPI applications to an MPI parallel analysis. Like our prototype, this approach uses MPI-based communication to forward event information for all MPI operations to an analysis component. Both Paradyn and TAUoverMRNet collect profiling data and employ the Tree-Based Overlay Network (TBON) infrastructure MRNet [13] to aggregate this information. Periscope records summary information from hardware performance counters to automatically detect known performance inefficiencies. It requires a socket-based TBON implementation for data collection and forwarding. Periscope does not forward event data. Scalasca is an event-based performance analyzer for well known performance problems with a focus on MPI. The tool records event traces to disc that are automatically analyzed during a replay of the data after the completion of the program. Therefore, strictly speaking Scalasca is on the threshold to an online tool. Finally, SSB applies a statistical analysis on sampled hardware performance counters to associate performance inefficiencies with processor components.

The monitoring approaches above mostly address profiling data, i. e., data is reduced to aggregated information, whereas we propose a workflow that can capture entire event traces for detailed and graphical analysis. While Periscope and SSB use basic application steering capabilities to drive their automated analysis, none of them enable the wide range of use cases that we envision for interactive online performance analysis at the exascale. Scalasca and TAU-VNG offer an event tracing workflow, i.e., keep all events during runtime for analysis. This overall approach largely resembles our design, but differs in major characteristics: Firstly, it specifically uses I/O components to synchronize trace recording and analysis while our approach avoids the use of this subsystem due to expected decreases in I/O bandwidths for future systems, and secondly, the trace size that is created is unbound and could make trace analysis with low memory per core ratios impossible.

Low memory per core ratios require an efficient management of trace data, which has been previously addressed by OTF2 [14], cCCG [15], a study of reduction techniques [16], and ScalaTrace [17]. While the latter three approaches are capable of reducing the trace data to a nearly constant trace size (depending on the granularity of the aggregation) they may be very time consuming1. Therefore, they can perturb application behavior to a degree that disables meaningful performance analysis. The MPItrace extension presented in [7] uses cluster and spectral analysis to reduce the number of events in traces and, thus, the trace sizes. This approach forwards and aggregates online performance data with MRNet and analyzes it on a front-end. The results are then broadcasted via MRNet to the measurement nodes, which use the information to selectively record events. While this approach efficiently enables a single online use case with a hard-coded steering technique, this approach still does not limit event data to a fixed size, which is crucial for a pure in-memory analysis.

# IV. INFRASTRUCTURE

All event-based performance analysis tool sets that we are aware of, including tools such as Scalasca, TAU, and Vampir, follow the principle workflow of Figure 1(a). Events generated during the application runtime, e.g., entering and leaving a function or communication between processes, are recorded by a measurement environment and stored in internal memory buffers. At the end of the application run, the data is stored in a file system; usually in one file per processing element. In addition, whenever the internal memory buffers are exhausted during runtime, the data is written to the file system, as well. After the application is terminated, the stored event trace is read in by an analysis tool, which then provides the analysis results.

In the online event-based performance analysis workflow that we propose (Figure 1(b)), the file system is eliminated in order to achieve the goals that we describe in Section I. Without the filesystem, alternative realizations need to be found for the following essential workflow parts: first of all, we need a direct coupling between the measurement environment and the analysis tool. Secondly, buffer overflows during the collection of event data now need to be handled online within strict memory limitations by means of effective data reduction and compression. Furthermore, the analysis tool needs to interact with the measurement environment to enable an interactive online workflow as we describe in our use cases (Section II).

The workflow in Figure 1(b) resembles the workflows of tools such as MALP, Periscope, and TAUoverMRNet. Yet, these tools store profiling information rather than individual events as we do. The following section outlines how we couple the measurement system and the event analysis engine without the need for a file system. In the section thereafter, we describe our event data compression and reduction management within a fixed-size memory footprint.

1The approaches in [16] and [17] do not give results for runtime overheads.

![](_page_2_Figure_7.png)

(a) Traditional event-based performance analysis workflow: Events generated by the application are recorded with a measurement tool and stored at the file system. Post mortem the data is read in by an analysis tool, which provides the analysis results.

![](_page_2_Figure_9.png)

(b) An online event-based performance analysis workflow keeps event trace data in memory and eliminates file system interaction during runtime. Optionally, analysis results may be stored persistantly for future reference.

Fig. 1. The principle tool design and workflow for traditional and online event-based performance analysis approaches.

#### *A. Coupling of Measurement and Analysis*

A measurement-analysis coupling for an online event-based analysis tool must provide the following capabilities:

- An instrumentation component that perceives relevant application events such as communication operations or function calls,
- A location at which event traces reside, e.g., directly on application threads or on tool-dedicated compute resources, and
- A communication system to forward event information from application threads to the locations that host event trace data, that allows data exchanges for actual event analysis, and for communication with a user interface.

![](_page_2_Figure_17.png)

Fig. 2. Tool layout for GTI-OTFX with eight application threads (0–7), four tool places that manage event trace information (T0–T3), three tool places that handle data analysis (A0–A2), and a graphical user interface (UI).

To provide these capabilities, we make use of key technology that has already proven its efficacy in existing tools. The corresponding software architecture is depicted in Figure 2. Perturbation is a critical issue when monitoring software. To limit perturbation, application threads should not perform monitoring tasks wherever possible. Additionally, event trace data should not reside on the application threads as this implies less available memory for them. Therefore, we use additional tool-dedicated compute resources, e.g., as extra threads or processes, that we call *places*. Figure 2 (left) illustrates application processes as nodes with labels 0–7. We deploy one hierarchy layer of places (nodes with labels T0–T3) to hold event data that if forwarded from the application processes. Further hierarchical layers of places (nodes with label A0–A2) are used to condense the information during our event analysis. An instrumentation system on the application processes captures events and forwards them to the first layer of places. Related tools such as MALP and MUST [18] demonstrate that this type of event data forwarding and processing is feasible both cases demonstrate this for MPI communication operations. Aside from that, we allow to steer the instrumentation systems on the application processes with feedback from our event analysis or with user commands, e.g., from a graphical user interface (node with label UI). Approaches such as the MPItrace extension [7] document the feasibility of such an instrumentation system control.

For the event analysis we rely on a hierarchy of places that represents a TBON. Such a topology can efficiently implement scalable data analysis with aggregation concepts that condense events as they progress from lower layers towards the root of the network, i.e., a user interface. Approaches such as TAUoverMRNet [9], Periscope [6], STAT [3], and MUST [18] highlight the efficiency of this aggregation concept and demonstrated scalability for more than one million processes2. This concept applies to our performance analysis use case as well. Since a user interface can only visualize a fixed amount of information on a screen of finite resolution the presented information needs to be decoupled from the actual application scale. Aggregations in a TBON can condense this information and provide application scale independent throughput requirements. Successful experiments with the distributed VampirServer [19] performance visualizer (offline) underline this approach. To enable an interactive search for relevant performance information, the Vampir tool suite uses repeated event analysis queries to react to user input, e.g., to provide details for a certain time interval or for specific processes. Likewise, we use repeated queries and aggregations in our TBON to enable equivalent searches. While the beforementioned TBON tools do not use such repeated and user driven queries yet, they are common and well tested in the scope of debuggers like Allinea's DDT [20].

## *B. Data Management During Runtime*

The nature of event tracing is to provide very detailed information by collecting and storing runtime events such as function entry/exit or sending and receiving a message. Information about these events is stored in event records within internal memory buffers. Although these event records themselves are rather small, they are typically recorded at very high rates. This regularly results in huge generated data volumes that overwhelm the memory buffer capabilities. In traditional event tracing approaches this situation causes a socalled buffer *flush*, which writes event data to the file system, or even to a termination of the measurement. However, for an online event-based performance analysis workflow it is crucial to keep the recorded event trace data in main memory for the entire measurement, as to completely eliminate file system interaction. Whereas, the memory buffer is not supposed to grow in size compared to traditional event tracing approaches, but remains at a small size to provide most of main memory for the application. Therefore, a key challenge in online eventbased performance analysis is to apply techniques that can manage event trace data of arbitrary size within a fixed-sized memory buffer.

We think of three key steps to meet this challenge. The first step consists of intelligent high-level filters and application phase-based selection. Thus, only relevant events are stored. This step involves runtime techniques like automated filters to reduce the recording of very short and highly frequent function calls that flood the trace buffers or, in iterative codes, the adaptive selection of only a few representative iterations, to minimize redundancy [21], [7].

The second step is efficient storage of the collected event trace data. The enhanced encoding techniques in [22] showed a remarkable increase in memory efficiency by a factor up to 5.8 without increasing the overhead of the event-tracing library. This matches the memory efficiency of well-established compression libraries without their additional overhead.

While the first two steps can achieve a tremendous reduction of the stored data they lack the capability to reduce the data to a fixed size, i.e., if the input data is large enough they can not keep the internal memory buffer from overflowing. Therefore, the last step must reduce whatever amount of data that is left, to the fixed size of the recoding memory buffer, guaranteeing that any measurement fits into a single memory buffer. This last step is triggered whenever the internal memory buffer is exhausted; usually this is the point where the memory buffer is either flushed to a file or the measurement is aborted. The main task of this third step is to transparently make space available again by reducing the number of events stored in the memory buffer. A selection of strategies to efficiently select and reduce events in the memory buffer is discussed in detail in [23]. Such strategies include for example a spatial reduction by time, a reduction based on the type of runtime event, or a gradual reduction based on the calling context. With these strategies it can be ensured that the collected event trace data can be efficiently reduced whenever the memory buffer is exhausted, thus making new memory space available for further events and avoiding interaction with the file system at any case.

For our infrastructure we use a prototype implementation called OTFX, which is an in-memory extension to the Open Trace Format 2 (OTF2) [14]. It implements some of the automated runtime filters in the first step, the enhanced encoding techniques from step two, as well as a hierarchical memory buffering data structure to efficiently apply the event reduction strategies in step three [24].

<sup>2</sup>https://www.llnl.gov/news/newsreleases/2012/Nov/NR12-11-02.html

# *C. Discussion*

In summary, the coupling architecture that we propose carefully combines well tested concepts to enable user control, low application pertubation, and scalability at the same time. In combination with OTFX, we can store and analyze event data online, while we can completely avoid the use of the I/O system. This differenciates our approach from tools such as MALP, Paradyn, Periscope, and TAUoverMRNet that also operate online, but that only store profiling data. Especially, for long running applications that motivate Use Case B from Section II, such event data could provide the necessary detail to understand impact of fine-grain events on global application behavior. Event information provides a high degree of flexibility for performance analysis, especially, with a plug-in mechanism as in Use Case F. The tools Scalasca and TAU-VNG both store event traces, but rely on the I/O system to manage this data. Scalasca purely analyzes the data after the application run, i.e., in its current implementation more resembles a post-mortem tool, while TAU-VNG also targets an online workflow. TAU-VNG also uses a far more coarsegrained coupling that demonstrated little steering capabilities of the instrumentation system, e.g., to reflect the results of event analysis or to react to user input as in Use Cases B–D from Section II. As a result, to the best of our knowledge, our architecture enables a first online workflow for event-based performance analysis in HPC that avoids the use of the I/O system.

#### V. PROTOTYPE

We implement a software monitor for online trace analysis as a prototype called GTI-OTFX. To realize the architecture in Figure 2, it applies the OTFX library and the tool infrastructure GTI. OTFX serves as constant in-memory event management facility, while GTI provides the application coupling along with the monitor/instrumentation components. This prototype targets a performance evaluation of the data collection and application steering functionality. Particularly, we want to explore the feasibility of an online event collection with a TBON tool layout. Abilities to control the instrumentation and influence the application during its runtime are the second goal of our evaluation to explore the feasibility of user-driven online trace analysis use cases.

Our prototype targets MPI applications due to their wide use on HPC systems. Support for other paradigms primarily requires extensions for the instrumentation system. Tools such as Scalasca and Vampir demonstrate that event-tracing workflows can support wide ranges of parallel programming paradigms in practice. With MPI as the target paradigm, nodes 0–7 in Figure 2 represent the application's MPI ranks. GTI uses an MPI communicator virtualization technique to partition the set of all MPI processes into subsets. The first represents the application processes and the remaining ones layers of tool places, e.g., T0–T3 and A0–A3 in Figure 2. This technique allows us to spawn tool processes by specifying additional processes to the *mpiexec* command and requires no recompilation or source modification of the target application. OTFX traces reside on the first layer of tool places and use configurable buffer sizes. These places manage one OTFX trace (event stream) for each application process that connects to them. GTI provides the actual instrumentation for GTI- OTFX and currently instruments all MPI-1 functions along with hooks to instrument user functions.

Since both application processes and tool places are processes of one MPI application, we can utilize very efficient MPI communication for the tool internal communication, which overcomes bandwidth, latency, portability, and flexibility limitations of socket-based TBON infrastructures. Thus, all edges in the topology of Figure 2 represent MPI-backed communication, except for the connection between the root of the TBON (A2) and the graphical user interface (UI), which uses a socket connection. Additionally, GTI provides flexible control over the use of a communication medium to select when it transfers performance data. The communication between the application processes and the processes that store event data with OTFX (dashed lines in Figure 2) concatenates multiple events into larger continuous buffers (100 KiB) for bandwidth efficient communication. The hierarchical, latency efficient communication channels towards the UI (fine-dashed lines in Figure 2) immediately forward partial performance results to the UI, which enables low response times for users.

We built a QT-based [25] graphical user interface to investigate the steering features in GTI-OTFX. Our implementation includes: pausing application processes directly after they invoke MPI_Init, pausing the application during its execution, resetting OTFX buffers, disabling event recording, and recording events for a given time period. This key functionality enables the Use Cases A and B from Section II and shows acceptable latency. To support Use Case C, we already have the necessary communication and steering infrastructure to provide this information to application processes in a scalable manner. Extensions of our current instrumentation capabilities could then enable a fine-grain control over their state and configuration. For a performance counter event source, such a reconfiguration is easy to achieve, while disabling/enabling specific function instrumentation requires just in time instrumentation capabilities as provided by Dyninst [26]. Support for Use Cases D and E requires extensions of our implemtation to provide an API that highlights user-defined conditions and to reuse trace writing techniques of trace libraries such as OTF2 [14]. Finally, our development approach with the parallel tool infrastructure GTI allows tool users to integrate with our event analysis capabilities and to let them provide plug-ins for analysis as in Use Case F. Current capabilities of GTI enable this at tool start-up time and extensions of GTI could provide hot-pluggability in the future.

#### VI. EVALUATION

We use Sierra, a Linux cluster at the Lawrence Livermore National Laboratory, to evaluate online event tracing overheads with GTI-OTFX. This system consists of 1,944 nodes of two 6 core Xeon 5660 processors and 24 GB of main memory each. We select the NAS Parallel Benchmarks (NPB) [27] (v3.3) at problem sizes D as a strong scaling benchmark. The experiments stress the instrumentation, event forwarding, and trace compression components of GTI-OTFX, which must provide low overheads as to limit application perturbation. A second goal of our measurements is to evaluate memory requirements for OTFX for this benchmark to draw conclusions for the feasibility of online event tracing workflows on post-petascale systems.

![](_page_5_Figure_0.png)

Fig. 3. GTI-OTFX prototype overhead and benchmark event rates for the NAS Parallel Benchmarks on the Sierra Linux Cluster at the Lawrence Livermore National Laboratory.

Figure 3(a) presents application slowdowns for all kernels and pseudo applications of NPB. We calculate the slowdown as the ratio between the reported benchmark times for GTI-OTFX and a reference run. We execute both runs in the same batch job and allocate application processes of both runs onto the same compute cores. Tool overhead is generally low (considering a prototype) except for the kernel sp at 2,048 processes for which we identified inefficient MPI_Request handling in GTI-OTFX as a primary overhead source. Additionally, runtime with GTI is below the reference time for lu. We saw similar behavior for the GTI-based tool MUST [28] and identified that tool communication reduced the numbers of outstanding MPI_Send operations for this pseudo application. We assume that lower amounts of outstanding sends reduce overheads within the MPI implementations (to satisfy the MPI progress property [12]) and could support this theory with a wrapper that replaces some MPI_Send operations by MPI_Ssend operations. Figure 3(a) also highlights that the tool overhead increases with increasing MPI call rates (Figure 3(c)). In our benchmark configuration, we use tool layouts where each first tool layer place serves 8 application processes and all remaining nodes of the TBON serve 16 places. Thus, at the peek call rate of more than 35,000 MPI calls per process and

second for lu, each first tool layer place must handle events for more than 280,000 MPI calls. Most calls generate three events, an enter event, a leave event, and a point-to-point communication event. Figure 3(d) provides function call rates for the kernels of NPB that highlight that their rate can exceed MPI operation rates. Most tracing tools apply filter rules to reduce the impact of high function rates.

Besides tool overhead, an important feasibility characteristic for online event tracing workflows is their memory usage. Projections for exascale systems suggest systems with 22 MiB per core (aggressive straw man projection) [2] while current Xeon Phi architectures provide less than 150 MiB per core. Figure 3(b) provides average write rates (per process) to the OTFX traces for all NPB kernels that exhibit higher MPI call rates. The kernel lu has the highest write rate with about 5.5 MiB per second and process. Thus, if applications on an exascale level feature event rates as for our experiments, an online trace analysis tool could at least record multiple seconds of execution time with complete event streams. Additionally, the event reduction capabilities of OTFX provide methods to gradually reduce the recorded detail to manage data for longer runs.

# VII. CONCLUSIONS

In this paper we argue for an online event-based performance analysis workflow to meet the challenges of future HPC systems, such as less memory per core, limited I/O capabilities, and increasing core numbers. We define motivational use cases that underline the benefits of such an approach and support our initial hypothesis that monitoring and analysis need to become a fully integrated online process on exascale systems. We propose an architecture that combines a hierarchical and scalable coupling of measurement and analysis components and an all-in-memory tracing approach based on efficient encoding and event reduction strategies. While the design efficiently avoids scalability limits in current and future system designs, it also enables new use cases that provide more efficient performance optimization workflows. Our highly interactive use cases require less and shorter iteration steps to recognize and isolate performance issues and their cause. Additionally, for long running applications, user predefined conditions or time intervals allow detailed tracing of short (but important) program phases.

We implement a workflow prototype that combines the GTI tool infrastructure and the OTFX trace management library. This prototype focuses on instrumentation, event processing, and application steering features to validate our design. This enables use cases such as recording application behavior for certain time frames or turning on and off the creation of MPI events while the application is running. The resulting setup allows us to study the agility of the proposed user interface driven trace analysis regarding its application and instrumentation steering capabilities. We evaluate our approach with the NAS Parallel Benchmarks at up to 2,048 processes and observe reasonable overheads for the prototype implementation. Additionally, we closely monitor event rates of this benchmark to draw conclusions towards online trace analysis on exascale-level systems. Our results indicate that even very low memory per core ratios permit lossless event tracing for time ranges of multiple seconds. In addition, the lossy event reduction capabilities of OTFX offer much longer observation intervals at the cost of a reduced level of detail.

With the proposed approach online event-based performance analysis that entirely avoids the I/O subsystem and provides interactive analysis methods becomes feasible for the first time. This forms a key step towards efficient performance analysis that supports the software development on future exascale systems.

# ACKNOWLEDGMENTS

This work has been supported by the CRESTA project that has received funding from the European Communitys Seventh Framework Programme (ICT-2011.9.13) under Grant Agreement no. 287703. We thank the ASC Tri-Labs and the Los Alamos National Laboratory for their friendly support.

#### REFERENCES

- [1] Top500, "Top 500 supercomputer sites," http://www.top500.org/, 2014.
- [2] K. Bergman, S. Borkar, D. Campbell, W. Carlson, W. Dally, M. Denneau, P. Franzon, W. Harrod, J. Hiller, S. Karp, S. Keckler, D. Klein, R. Lucas, M. Richards, A. Scarpelli, S. Scott, A. Snavely, T. Sterling, R. S. Williams, K. Yelick, K. Bergman, S. Borkar, D. Campbell, W. Carlson, W. Dally, M. Denneau, P. Franzon, W. Harrod, J. Hiller, S. Keckler, D. Klein, P. Kogge, R. S. Williams, and K. Yelick, "ExaScale Computing Study: Technology Challenges in Achieving Exascale Systems," 2008.
- [3] D. C. Arnold, D. H. Ahn, B. R. de Supinski, G. L. Lee, B. P. Miller, and M. Schulz, "Stack Trace Analysis for Large Scale Debugging," *International Parallel and Distributed Processing Symposium*, 2007.
- [4] R. Azimi, M. Stumm, and R. W. Wisniewski, "Online Performance Analysis by Statistical Sampling of Microprocessor Performance Counters," in *Proceedings of the 19th Annual International Conference on Supercomputing*, 2005, pp. 101–110.
- [5] H. Brunst, A. D. Malony, S. S. Shende, and R. Bell, "Online Remote Trace Analysis of Parallel Applications on High-Performance Clusters," in *High Performance Computing*, ser. Lecture Notes in Computer Science. Springer, 2003, vol. 2858, pp. 440–449.
- [6] M. Gerndt, K. Furlinger, and E. Kereku, "Periscope: Advanced Tech- ¨ niques for Performance Analysis," in *Parallel Computing: Current & Future Issues of High-End Computing, Proceedings of the International Conference ParCo 2005*, ser. John von Neumann Institute for Computing Series, vol. 33, 2005.
- [7] G. Llort, J. Gonzalez, H. Servat, J. Gimenez, and J. Labarta, "On-line Detection of Large-scale Parallel Application's Structure," in *Parallel Distributed Processing (IPDPS), 2010 IEEE International Symposium* on, 2010, pp. 1–10.
- [8] B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam, and T. Newhall, "The Paradyn Parallel Performance Measurement Tool," *Computer*, vol. 28, no. 11, pp. 37–46, 1995.
- [9] A. Nataraj, A. D. Malony, A. Morris, D. C. Arnold, and B. P. Miller, "A Framework for Scalable, Parallel Performance Monitoring," *Concurrency and Computation: Practice and Experience*, vol. 22, no. 6, pp. 720–735, 2010.
- [10] J.-B. Besnard, M. Perache, and W. Jalby, "Event Streaming for Online Performance Measurements Reduction," in *42nd International Conference on Parallel Processing (ICPP)*, 2013, pp. 985–994.
- [11] M. Geimer, F. Wolf, B. J. N. Wylie, E. Abrah ´ am, D. Becker, and ´ B. Mohr, "The SCALASCA Performance Toolset Architecture," in *Proc. of the International Workshop on Scalable Tools for High-End Computing (STHEC), Kos, Greece*, June 2008, pp. 51–65.
- [12] Message Passing Interface Forum, "MPI: A Message-Passing Interface Standard, Version 3.0," http://www.mpi-forum.org/docs/mpi-3.0/mpi30 report.pdf, 2012, last visited on 27/11/2013.
- [13] P. C. Roth, D. C. Arnold, and B. P. Miller, "MRNet: A Software-Based Multicast/Reduction Network for Scalable Tools," in *Proceedings of the 2003 ACM/IEEE conference on Supercomputing*, ser. SC '03, 2003.
- [14] D. Eschweiler, M. Wagner, M. Geimer, A. Knupfer, W. E. Nagel, and ¨ F. Wolf, "Open Trace Format 2: The Next Generation of Scalable Trace Formats and Support Libraries," in *Applications, Tools and Techniques on the Road to Exascale Computing*, ser. Advances in Parallel Computing, vol. 22, 2012, pp. 481–490.
- [15] A. Knupfer and W. E. Nagel, "Compressible Memory Data Structures ¨ for Event-based Trace Analysis," *Future Gener. Comput. Syst.*, vol. 22, no. 3, pp. 359–368, 2006.
- [16] K. Mohror and K. L. Karavanic, "Evaluating Similarity-based Trace Reduction Techniques for Scalable Performance Analysis," in *Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis*, ser. SC '09, 2009, pp. 55:1–55:12.
- [17] M. Noeth, P. Ratn, F. Mueller, M. Schulz, and B. R. de Supinski, "ScalaTrace: Scalable Compression and Replay of Communication Traces for High-performance Computing," *J. Parallel Distrib. Comput.*, vol. 69, no. 8, pp. 696–710, Aug. 2009.
- [18] T. Hilbrich, F. Hansel, M. Schulz, B. R. de Supinski, M. S. M ¨ uller ¨ , W. E. Nagel, and J. Protze, "Runtime MPI Collective Checking with Tree-Based Overlay Networks," in *Proceedings of the 20th European MPI Users' Group Meeting*, ser. EuroMPI '13, 2013, pp. 129–134.
- [19] H. Brunst, W. E. Nagel, and A. D. Malony, "A Distributed Performance

Analysis Architecture for Clusters," in *Proceedings of the IEEE International Conference on Cluster Computing*, 2003, pp. 73–81.

- [20] Allinea Software, "Allinea DDT," http://www.allinea.com/products/ddt/, last visited on 27/11/2013.
- [21] M. Wagner, J. Doleschal, W. E. Nagel, and A. Knupfer, "Selective ¨ Runtime Monitoring: Non-intrusive Elimination of High-frequency Functions," in *2014 International Conference on High Performance Computing & Simulation (HPCS)*, 2014, (to be published).
- [22] M. Wagner, A. Knupfer, and W. E. Nagel, "Enhanced Encoding ¨ Techniques for the Open Trace Format 2," *Procedia Computer Science*, vol. 9, pp. 1979–1987, 2012.
- [23] M. Wagner and W. E. Nagel, "Strategies for Real-Time Event Reduction," in *Euro-Par 2012: Parallel Processing Workshops*, ser. Lecture Notes in Computer Science. Springer, 2013, vol. 7640, pp. 429–438.
- [24] M. Wagner, A. Knupfer, and W. E. Nagel, "Hierarchical Memory ¨ Buffering Techniques for an In-Memory Event Tracing Extension to the Open Trace Format 2," in *Parallel Processing (ICPP), 2013 42nd International Conference on*, 2013, pp. 970–976.

- [25] J. Blanchette and M. Summerfield, *C++ GUI Programming with Qt 4*. Upper Saddle River, NJ, USA: Prentice Hall PTR, 2006.
- [26] B. Buck and J. K. Hollingsworth, "An API for Runtime Code Patching," *The International Journal of High Performance Computing Applications*, vol. 14, pp. 317–329, 2000.
- [27] D. H. Bailey, L. Dagum, E. Barszcz, and H. D. Simon, "NAS Parallel Benchmark Results," IEEE Parallel and Distributed Technology, Tech. Rep., 1992.
- [28] T. Hilbrich, B. R. de Supinski, W. E. Nagel, J. Protze, C. Baier, and M. S. Muller, "Distributed Wait State Tracking for Runtime MPI ¨ Deadlock Detection," in *Proceedings of SC13: International Conference for High Performance Computing, Networking, Storage and Analysis*, ser. SC '13, 2013, pp. 16:1–16:12.

