# I/O Scheduling Service For Multi-Application Clusters

Adrien Lebre, Guillaume Huard, Yves Denneulin Laboratoire Informatique et Distribution - IMAG
Montbonnot Saint-Martin, France firstname.lastname@imag.fr Przemyslaw Sowa Institute of Computer and Information Sciences Czestochowa University of Technology, Poland sowa@icis.pcz.pl Abstract**â€” Distributed applications, especially the ones being**
I/O intensive, often access the storage subsystem in a nonsequential way (stride requests). Since such behaviours lower the overall system performance, many applications use parallel I/O libraries such as ROMIO to gather and reorder requests. In the meantime, as cluster usage grows, several applications are often executed concurrently, competing for access to storage subsystems and, thus, potentially canceling optimisations brought by Parallel I/O libraries.

The *aIOLi* **project aims at optimising the I/O accesses within**
the cluster and providing a simple POSIX **API. This article**
presents an extension of *aIOLi* to address the issue of disjoint accesses generated by different concurrent applications in a cluster. In such a context, good trade-off has to be assessed between performance, fairness and response time. To achieve this, an I/O
scheduling algorithm together with a "*requests aggregator*" that considers both application access patterns and global system load, have been designed and merged into *aIOLi*. This improvement led to the implementation of a new generic framework pluggable into any I/O file system layer. A test composed of two concurrent IOR benchmarks has shown improvements on read accesses by a factor ranging from 3.5 to 35 with POSIX **calls and from 3.3 to 5**
with ROMIO, both reference benchmarks have been executed on a traditional NFS server without any additional optimisations.

## I. Introduction

I/O bottlenecks have always been a major issue in Computer Science and it is likely to continue as performances increase slower for I/O hardware than for CPU and memory. This gap is widened by the increasing use of HPC platforms and the growing number of parallel I/O intensive scientific application. In this context, the I/O subsystem is stressed both by the overall throughput requirement and the peculiar access patterns of parallel applications. For instance, disjoint requests delivered at the same time may generate disk head movements, one of the most time consuming operations in modern computers (approximately 9 ms). But, as shown by This work has been done within the ID laboratory jointly supported by CNRS, INPG, INRIA, and UJF and the LIPS project between INRIA and BULL Lab. Computer resources are provided by the grid5000 French experimental grid (http://www.grid5000.fr).

several studies [6], [17], parallel I/O accesses use recurrent determined patterns (based on stride parameters) that are good candidates for optimisations. This is different from the "database accesses" which depend on the selection criteria and are usually more sparse.

In this article, we focus on multiple concurrent applications that perform parallel I/O accesses to the storage subsystem.

This is a common situation on clusters since most batch schedulers [9], try to maximize the overall platform usage, leading to concurrent executions. In this case, each application generates its own recurrent parallel access patterns and the ones from distinct applications are interleaved due to concurrency. Thus, the I/O subsystem layer has to perform optimisations that take advantage of regularity of accesses from each application while balancing storage access between them. Our work focuses on non dedicated clusters where all applications are considered of equal importance. This translates into multiple optimisation criteria: throughput, fairness and response time.

I/O parallel accesses optimisation issues have been addressed by several researchers. The proposed solutions can be classified into two main categories:
- *Parallel File systems* [4], [21], [22] manage to exploit hardware capabilities while taking into account distributed file system constraints (coherency, fault tolerance, remote accesses, . . . ) as efficiently as possible. Most of these systems do not use I/O scheduling strategies as they are just built on schedulers located at block device layer (section II-C). At this low level, due to kernel and file system implementation (see section III-A) parallel applications information is not available and parallel I/O access patterns cannot be exploited for throughput optimisation.

- *Parallel I/O Libraries* are focused on parallel I/O aspects and portability constraints inside a single application. They use a specific API to enable the developer to express I/O access patterns. The underlying optimisation algorithm exploits these patterns to aggregate individual 1-4244-0328-606$20.00 c 2006 IEEE. requests into larger ones (see section II-A). Unfortunately, beside the complexity of the API of these systems, storage access balance between applications is not addressed by I/O libraries. Moreover, individual applications' optimisations are likely to be canceled by the interleaving of concurrent accesses made by distinct applications (see multi-applicative MPI I/O tests in section IV).

When dealing with concurrent parallel applications, it is important to address requests in a global manner to provide a good trade-off between performance and balance between applications. The performance itself could be optimised by treating all the I/O requests from one application before serving another one. This provides a good throughput but starves the waiting applications. Such a policy does not take into account response time and fairness criteria which are mandatory in a multiple applications environment.

We propose to design a high level infrastructure made of two main components: a transparent parallel I/O aggregation mechanism for throughput optimisation and a scheduling algorithm that balances service provision between applications.

Our main contribution is to integrate these two elements into a generic framework pluggable into any existing I/O system.

This framework does not require any change in the application code (it just interacts with the I/O subsystem). It only requires a minimal change in the I/O layer code to subscribe to the aIOLi services (section III-B). To take advantage of a global view of all the accesses without any harm on scalability, *aIOLi* should be plugged into any already existing I/O centralization point.

We have implemented our proposal as a Linux module and evaluated it on a Network File System server. Even if NFS is not really suited to high performance I/O, it remains the standard configuration for small and medium sized clusters. The server has to deal with huge amounts of simultaneous requests, a good testbed to evaluate the interest of a high level scheduler like ours.

The rest of our paper is organized as follows: section II
briefly presents the available I/O optimisations and their limits.

Section III is focused on the architecture of our framework: the interest of a higher level scheduler, the architecture of the current version of *aIOLi* and the two integrated mechanisms are introduced. Section IV gives some experimental results. Eventually, section V describes future extensions as well as possible improvements and section VI concludes the paper.

## Ii. Background

In this section, we present main ideas used in parallel file systems, in parallel I/O libraries or both. Most of them aim at optimising I/O throughput for one parallel application (collective approaches, prefetch) while others do not take advantage of accesses regularity (scheduling). The goal of this section is to give an overview of I/O optimisations and explain why they are not suited to the multi-applicative context.

## A. Collective Approaches

Different processes of a parallel application usually send many small, non-contiguous requests simultaneously to the I/O
server without checking for aggregation opportunities. Collective I/O methods solve this problem by merging different requests into a bigger one and issuing an aggregated request. This concept can be applied at different layers in a distributed architecture: at disk level [12], at server side [23] or at client side [25].

The resulting performance of collective approaches depends on the underlying architecture (middleware, network interconnection and file system implementation) and the use of specific "tuning" routines (MPI I/O *hints* functions) is often required. Furthermore, such approaches imply expensive synchronisation mechanisms [1]. In the case of writes, Ma et al. has proposed to use active buffering with threads [15] to efficiently overlap I/O with computation. This lessen the effect of synchronisation but requires a modified ROMIO library.

Overall, the collective approaches do not take the interleaving of non contiguous access resulting from concurrent execution of several applications into account. As shown in section IV, this leads to severe performance degradation.

## B. Cache And Prefetching

Caching is a widespread technique used to reduce the number of accesses to hard drives and thus improves performance of the I/O system [3]. In centralized/local systems, cache management is not very difficult, it differs significantly in distributed and parallel environments where strict coherency protocols are complex and impact performance. Some systems avoid this problem by sacrificing the client side file caching and keeping caching only on I/O nodes. Collective caching
[27] is another method to improve I/O performance of parallel applications. It is based on the idea that all processes running the same application should be considered as a single client of a parallel file system. A modified MPI version provides user-level file caching by distributing the cached data equally between processes. Unfortunately, this mechanism requires a first step, where data are redistributed to different nodes to build the cache system. This step can be expensive for large workloads.

Moreover, several levels of cache can lead to inefficiency if, for each cache request, a cache miss occurs [26] . Overall, caching can be combined with *aIOLi* but it does not solve the same problem: multi-applications balancing and initial fetch of data are not addressed.

Prefetching techniques are based on implicit or explicit anticipation of I/O requests. Some parallel intensive I/O programs reduce I/O congestion by retrieving explicitly data in a sequential way from one client (in a synchronous or asynchronous mode) and redistribute them to all participants.

Such an approach leads to good performance in a nonconcurrent environment thanks to the gain provided by the Read Ahead technique[7]. Daniel Ellard and Margo Seltzer
[8] have modified the FreeBSD 4.6 NFS server to improve the read-ahead heuristic strategy. Their new algorithm handles stride access patterns in a better way.

Unfortunately, in multi applications environments, these prefetching methods generate parallel I/O problems. As in the case of collective approaches, they do not take the interleaving of non-contiguous access resulting from concurrent execution of several applications into consideration.

## C. I/O Scheduling Strategies

Several scheduling algorithms have been proposed to minimize the completion time of a batch of I/O operations. These algorithms usually fall into two categories: disk scheduling [24] and parallel I/O scheduling [5], [11]. The first one tends to limit disk head movements while the other one distributes parallel I/O operations to different I/O servers to minimize the overall response time.

Disk scheduling algorithms, because they are all implemented at a low level, cannot have a good overview of distributed applications accessing the file system. Besides, being limited by the size of their queues, they are strictly dependent on the implementation of the upper file system: for instance, if the file system is synchronous and mono-threaded, only one request can be handled at the same time which limits potential optimisations (section III-A for more details). As a direct consequence, such approaches are not suited to large workloads generated by HPC intensive I/O applications.

Parallel I/O scheduling, in contrast to low level schedulers, tries to exploit parallel I/O access patterns. In PVFS [20] a model for predicting performance of a system for a given workload is used. Based on this model, the system can choose dynamically the most appropriate scheduling algorithm. In the Clusterfile parallel file system [10], a scheduling heuristic tries to involve all the I/O servers in the system at the same time to maximize their utilization. However, to the best of our knowledge, these systems do not address the interleaving of multiple applications. Hence, they suffer from concurrent disjoint access resulting from the simultaneous execution of several applications.

Bandwith

![2_image_0.png](2_image_0.png)

## Iii. System Overview

The first implementation of the *aIOLi* prototype [13] has provided an efficient transparent management of parallel I/O
for one application within a SMP node. All I/O requests, before being sent to a remote server, have been analyzed, to find aggregation possibilities, and reordered to favor sequentiality. The encouraging results of this first implementation have motivated the study of similar approaches but at cluster level [14]. In this intermediate work, we have chosen a client/server model comparable to the PANDA architecture [23]. Unfortunately, the lack of global memory and global clock makes the management difficult and the expected improvements have not been reached.

In this new study, we have opted to move *aIOLi* from the applications to the storage system layer to collect more information about both applications and global system load.

Indeed, one of the main ideas of this new proposition is to exploit the existing centralization point of file system architectures to plug our new framework. Therefore, *aIOLi* does not compromise the I/O system scalability by adding new bottlenecks.

## A. Preliminary Study

As mentioned in former sections, low-level scheduler does not have a global view of the I/O accesses made by a parallel application. This is due to limitations in the queues' size and in the I/O server implemented on top of them.

We have checked aggregation capabilities of these schedulers using the IOR benchmark presented in section IV-C. The experiments consists in evaluating IOR over 32 MPI instances deployed on 32 nodes and decomposing a 4GB file on a Linux NFS server (platform details are given in section IV).

A test using the POSIX API has been performed for each Linux I/O low level scheduler. The file access granularity has varied from 8KB to 4096KB. To provide more aggregation opportunities to the scheduler, the number of active nfsd daemons ranged from 8 (default configuration) to 512 (more incoming simultaneous requests). The results are similar regardless which I/O scheduler is used, we only present the default one: anticipatory scheduler on the right of figure 1 (the results for all the other schedulers can be found on the aIOLi website http://aioli.imag.fr).

As we can notice, the more daemons, the better the performance. Nevertheless, even with 512 daemons, the low-level scheduler's performance is far from sequential performance (around 50MB/sec) in most cases.

## B. A High Level I/O Scheduling Framework

The purpose of the *aIOLi* framework is to offer some generalized I/O scheduling strategies independent of any storage medium or I/O subsystem. The latest version of *aIOLi* can extend almost any existing I/O management system using a simple plug-in mechanism.

Figure 2 shows the architecture of the framework. *aIOLi* client can be the whole kernel I/O subsystem, a single remote file system or any other I/O intensive service. Each client is connected to an *aIOLi* I/O controller. Every *aIOLi* client should be able to react to the two following events:

![3_image_0.png](3_image_0.png)

- a new request is delivered to the client. In this case, the client should post this new request to the queue of the related *aIOLi* I/O controller.

- the *aIOLi* I/O controller notifies the client that some of its requests can be processed. The client then should process it.

We have chosen to let the clients process themselves the requests they posted. This way, *aIOLi* remains generic and independent, it just acts as an aggregator and a scheduling service.

An *aIOLi* I/O controller can be in charge of scheduling requests from one or several clients at the same time. For instance, on a remote NFS server which exports an Ext3 partition, an *aIOLi* I/O controller can be used to schedule both requests incoming to the NFS server and local requests to the file system. In this case, the NFS server and the Ext3 file system will be two clients associated to the same I/O
controller. Such an approach allows *aIOLi* to be plugged where needed (on an independent file system or on the I/O layer of the whole system) to perform fine optimisations at the appropriate level.

An *aIOLi* scheduler has to be chosen when initializing an I/O controller. These schedulers exploit file id, request size and start offset to choose the appropriate issue order. Thus, they deeply differ from low level schedulers mainly based on disk sector placement (section II-C). Currently, *aIOLi* ships with two different schedulers : a simple FIFO and a MLF variant discussed in section III-D. Additional scheduling algorithms are easy to add either directly to *aIOLi* or as an external kernel module.

Regarding coherency, to prevent any issue, an unique timestamp is associated to each incoming request. Using these timestamps, the strict order between write and read accesses for the same resource can be enforced. Currently, all requests following a write to the same file are blocked by the system until the completion of this write. This method could be easily improved by using a finer locking mechanism such as the wellknown *Byte-Range-Locking*.

aIOLi, which was formerly a library, is now a pluggable I/O
scheduler. This change makes our framework extensible and flexible: it is completely independent from applications and can be plugged into any existing I/O management system.

aIOLi is now a high-level I/O scheduler : it only requires generic information for the requests it handles (offset and size). Our framework also includes a statistics collector which gathers information about I/O workers (number of requests proceeded, number of aggregations, average, etc.) either for post mortem or on-line analysis.

## C. Aggregation And Virtual Aggregation

The first prototype of *aIOLi* [13], was a library dedicated to the optimisation of I/O in a single node. Since we were within a single node (only one operating system), coherency mechanisms were provided by the underlying file system stack
(single buffer cache) and we implemented a physical aggregation mechanism: small contiguous requests were merged into a larger one which was sent by the kernel to the remote file system.

In a distributed environment, the implementation of such mechanisms becomes tedious (because of data replication, cache invalidation, etc.). Moreover, few file systems provide routines to access to a group of disjoint file parts (I/O vector).

To remain generic, *aIOLi* is able to perform its optimisation on the basis of simple access requests only. It uses"virtual aggregation" mechanism to choose an execution order for several requests, later the actual execution of I/O calls is done by the clients. For instance, the three following requests:
read(30,40), read(20,30), *read(10,20)*1, should be reordered and executed in the following order: read(10,20), *read(20,30)*, read(30,40). This way, all the accesses become contiguous.

When I/O systems provide specific routines to handle I/O
vectors, *aIOLi* will use them and thus apply a "real aggregation". But, our experiments show that the "virtual aggregation" mechanism is sufficient to reach near-optimal performance: the genericity does not imply any performance degradation.

## D. Scheduling Of I/O Requests On A Cluster

aIOLi provides scheduling strategies to share the I/O subsystem efficiently among all applications running on the cluster.

As mentioned in section I, in our case, we aim at providing scheduling strategies optimising throughput first, but with a concern for fairness and response time.

1) Base Scheduling Algorithm: We want both to maximize the overall performance by using parallel I/O aggregation techniques and maintain a good balance among applications. Unfortunately, throughput, fairness and response time are incompatible: to be fair and responsive I/O resource access has to be switched regularly between applications thereby breaking the contiguity of accesses required to reach the 1read(x,y): read from offset x to offset y in the same file maximal throughput. Thus, our scheduling strategies will consist in the best compromise between performing maximal aggregation and serving each application in turn. This I/O requests scheduling in our context is an on-line problem: jobs (requests) keep on arriving during the scheduling process and the total size of the access (total number of requests) is not known in advance [2].

The base of our algorithm is a variant of the Multilevel Feedback algorithm (MLF) [18]. The MLF algorithm is designed to optimise average response time while avoiding starvation by granting to individual waiting requests an adaptive time quantum for accessing resources (a quantum that grows with time). We have modified MLF in order to integrate the virtual aggregation mechanism into it. The resulting algorithm can be described as follows:
1) incoming requests are sorted by type (read or write) and inserted into two separate queues for each file accessed.

2) each request is assigned an initial quantum of zero upon its arrival in the system.

3) aggregation is performed on requests of both queues
(read and write): the queues are traversed in offset order and contiguous requests are aggregated into larger virtual requests which quantum is the sum of individual requests quantum.

4) the quantum of each request is then increased by a fixed value QB (which is rather small to favor interactivity).

5) the first request, in offset order within a file and FIFO
order between files, which quantum is large enough to enable its completion to be selected for execution.

Figure 3 presents the behaviour of our algorithm on a toy example made of two processes (A1 and A2) accessing a file
(file 1) in a stride like manner (with strides of 128K long) and two processes (A3 and A4) accessing two files (file 2 and file 3) in a synchronous way (one request after the other). In this example all the accesses are 32K long and QB equals 32K. As with MLF, the use of small quantum that increases with time gives priority to small requests (not aggregated). Thus, big virtual requests will remain for some time in the system giving it opportunities for more aggregation (in other words, throughput optimisation). Processing requests in the order of their sizes is the optimal strategy to minimize the average response time making our algorithm good for interactive tasks. Nevertheless, because we have bound aggregation and the quantum increases regularly, waiting aggregated requests will never starve and a decent fairness will be maintained. Notice that our algorithm is a compromise. Indeed, we could improve throughput by removing the quantum system and stick to the offset order, but it would degrade response time and fairness as requests would possibly be delayed for a long time by other ones.

2) Performance Tuning: Although the preceding algorithm has a good overall behaviour, optimising for several antagonist criteria at once, it is not able to detect and exploit some widespread specific accesses behaviour of common applications completely:

![4_image_0.png](4_image_0.png)

synchronous accesses applications such as cat make their access in a synchronous fashion, one byte at a time, waiting for the result before performing the next access.

In that case, it should be advantageous for the server to wait for the next access at the end of the request processing. It could be done by giving the request a quantum larger than required for its completion: the extra time can then be used to wait and aggregate consecutive requests as they arrive.

very large accesses most applications generally make either very large or very small accesses to files. In the case of large accesses, the linear increase rate of the quantum in the base algorithm does not exploit aggregation opportunities sufficiently. Thus, it should be advantageous to give a larger quantum to consecutive requests to the same file, as long as this quantum is fully exploited.

very small accesses In the case of small accesses, the quantum given to requests might be too large (especially for requests of size lower than the base quantum size or requests that have stayed too long in the system). The issue is that when we use this extra quantum duration to wait for aggregation opportunities, it only ends up in useless delay.

To address these issues, we take advantage of file accesses history and we adapt the quantum size to applications' specific behaviour dynamically. For each file accessed, we store the utilization rate of the quantum given to the last access. If this rate is high, we will be likely to perform a large or synchronous access. In that case we expand the size of the quantum given to requests to this file by a multiplicative factor. On the contrary, if this utilization rate is low, we will be likely to perform small accesses and we shall reduce the quantum size accordingly. As a special case, when the utilization rate is very low the end of the file should have been reached. In that case, we simply reset the history information.

## Iv. Experiments

Our testing system is a part of the machines from the
"grid5000" project located at the INRIA Sophia-Antipolis site. Each node, an IBM eServer 325, is composed of two AMD Opteron (2GHz) CPUs, 2GB RAM and a 80GB IDE
hard-drive (bandwidth estimated to 57MB/s by the hdparm command). The cluster is interconnected by a gigabit ethernet network. All nodes were running a Debian GNU/Linux system with a 2.6.15 kernel. A dedicated NFS server (version 3, TCP, 32KB read/write size, sync, cache disabled) on top of an ext3 file system and several client SMP nodes have been used.

In the first part, we have evaluated the overhead of aIOLi and its impact on scalability for both non HPC and HPC
workloads. Then, we focused our experiments on the multiapplication criteria.

## A. Implemented Aioli Clients

The *aIOLi* 's public interface is composed by three main functions (two optional complete the API). Any *aIOLi* client has to call the initialization function with at least two callback functions as arguments: the read and write functions from the host I/O system. These functions will be called when aIOLi decide on the execution of one ore more requests. The additional optional callbacks can be given to *aIOLi* to handle "real aggregation".

Unfortunately, the client code has to be slightly modified
(that is the I/O system layer): to redirect incoming request to aIOLi, it is necessary to add a post call. This is the only intrusive step when plugging *aIOLi* to a client. Up to now, we did not discover a completely transparent approach.

So far, two *aIOLi* clients have been developed. The first one is a NFS (Network File System) server based on the source code from the 2.6.15 Linux kernel. The second one is an extension of the Linux Virtual File System which handles all I/O operations on a single node. Both of them are shipped with the *aIOLi* source code distribution . Their evaluation led us to the same conclusion. Therefore, due to space considerations, only NFS experiments will be discussed in this article.

## B. Overhead And Scalability Impact

1) Bonnie++: Bonnie++2 is a popular and widely used benchmark to evaluate hard drive and file system performance. It tests writes, reads and creation of files. The write test consists of three phases : a file is first written byte by byte, then it is overwritten in a block manner and finally it is read and overwritten block by block. The two-phase read test is similar to the first two phases of write test. Although Bonnie++ is 2http://www.coker.com.au/bonnie++
often used as a benchmark for clusters, it is not really suited to this task because: no HPC application accesses data using a fine granularity (byte by byte) and stride accesses are not tested.

Nevertheless, we decided to benchmark *aIOLi* with Bonnie++ anyway to prove that our system has neither a negative impact on fine grained sequential performance, nor adds perceptible latency or overhead. Our test consists of two parts: the first one with one client, then with four clients. Both parts use one NFS server. We have configured Bonnie++ to use 4 GB files and to skip the file creation test. The results are presented in the table I.

Write Read char block rewrite char block MB/s MB/s MB/s MB/s MB/s NFS (1 client) 21.69 28.40 2.00 34.84 43.54 aIOLi (1 client) 19.80 28.29 2.03 37.45 48.81 NFS (4 clients) 8.26 8.78 1.68 3.55 3.02 aIOLi (4 clients) 7.41 9.59 1.68 4.74 13.39 TABLE I
BONNIE ++ EVALUATION
As we can notice, *aIOLi* does not have any significant impact on I/O accesses at a fine granularity. In this case, requests are mostly satisfied by the NFS cache because the server makes access of 32KB anyway (our rsize parameter).

Thus, *aIOLi* has no room for further optimisations. It does not benefit to sequential writes either as they are already handled asynchronously by the system. But, the performance is greatly improved when four concurrent clients make simultaneous read at a coarse granularity. In this case the adaptive quantum mechanism used in *aIOLi* (section III-D.2) shows its efficiency.

2) b_eff_IO benchmark: To test the impact on scalability of our system, we have used the Effective I/O Bandwidth Benchmark[19], which provides two tests: the first one evaluates the average I/O bandwidth achievable when using the MPI
I/O library and the second one gathers detailed informations depending on the access patterns and buffers length. The benchmark tests "first write", "rewrite" and "read" routines using several combinations for its parameters such as: various parallel access patterns (different kinds of stride like patterns),
collective / non collective accesses, aligned / unaligned accesses. In this test, similar to a single parallel application, the I/O operations are already optimised by the MPI I/O library and *aIOLi* is not likely to produce further improvements. The goal is only to see the impact of *aIOLi* on a heavily loaded system. The number of clients varied from 2 to 96 (a high load for usual NFS servers).

The results are presented in figure 4. Regarding the write performance, because the system handles write requests asynchronously, *aIOLi* has not many possibilities for improvements. As a consequence, the optimisations it provides are compensated by the overhead of the scheduler. The resulting performance is roughly the same and scalability is not degraded.

![6_image_0.png](6_image_0.png)

![6_image_1.png](6_image_1.png)

Usual NFS server (left) and NFS server plugged with aIOLi (right)
Regarding read performance, thanks to its scheduling strategy, *aIOLi* performs clearly better than the NFS server itself.

The slight performance degradation when the number of clients increases is due to the CPU cost of processing a RPC request as shown in [16]. We do not present results for the rewrite test which are similar to the read performance results.

Overall, this test shows that *aIOLi* does not have any negative impact on I/O system scalability and can even bring some improvements when aggregation opportunities still exist.

## C. Multi-Application Criteria

In this part, experimentations focus on parallel application and multi-application aspects. The I/O Stress Benchmark Codes3 has been exploited to emulate the behaviour of a parallel I/O intensive application. It consists in a parallel file system code developed by the Scalable I/O project at Lawrence Livermore National Laboratory. This parallel program performs parallel writes and reads to/from a file using the POSIX
or MPI I/O API and reports the throughput rates.

In a preliminary part, we check whether the parallel accesses are efficiently managed by our framework (aggregation for one application) or not. Indeed, this is necessary as our main objective is to manage I/O in presence of multiple and concurrent parallel applications efficiently. Then, we evaluate the impact of one application on another : 1./ we evaluate the degradation implied by one parallel I/O intensive application on a less I/O dependant program 2./ we analyse the behaviour of two concurrent I/O intensive applications. Finally, the performance of a traditional NFS Server and a server exploiting aIOLi is compared using a workload composed by ten distinct applications.

1) Multi-Node Coordination: In this experiment, one IOR
instance has been over 32 MPI instances deployed on 32 nodes nodes. The file size has been set to 4GB and the file access granularity ranges from 8KB to 4MB. The figure 6 presents the performance provided by the usual NFS server accessed with POSIX ("POSIX" curve), the usual NFS server accessed with collective MPI I/O API ("MPI-I/O" curve) and the POSIX API on top of an NFS server plugged to the *aIOLi* framework ("aIOLi" curve).

On this test, MPI I/O has been run without file view but the results with file view are similar. The "POSIX-Ref" curve has been obtained by prefetching the data from one node. From the point of view of the NFS server, this matches a large sequential synchronous access (like a cat). The dashed line is the hard drive sustained bandwidth (obtained with the hdparm command), it is an upper bound for the performance of our NFS server. NFS Version 3 provides two modes for write access: synchronous and asynchronous, both were evaluated. Nevertheless, we only present results for the asynchronous write mode as the performance in synchronous mode is poor for all the compared systems and is not suited ![6_image_2.png](6_image_2.png)

Bandwith
4 GB file decomposition on over 32 MPI instances.

Regarding the read performance, thanks to its adaptive algorithm, *aIOLi* provides clearly better performance than POSIX and MPI I/O. It even surpasses POSIX-ref when a sufficient number of aggregation opportunities exists. The only situation when POSIX-ref performs better is when the granularity is between 64KB and 512KB : greater than the NFS access granularity (32KB in our case). In this case, each client access is resolved by very few NFS requests and because of concurrency these requests are disjoint. Because of the low number of requests, the scheduling window on which *aIOLi* works is too small and optimisations are limited. The problem does not appear neither with fine granularities (thanks to *aIOLi* offset order policy) nor with coarse granularities (because the scheduling windows is large enough to find aggregation opportunities).

![7_image_0.png](7_image_0.png)

4 GB file decomposition on over 32 MPI instances.
Regarding the write performance, once again *aIOLi* performs better than POSIX and MPI I/O. In this case its performance is very close to the performance of POSIX-ref.

This is because POSIX-ref writes in asynchronous mode and benefits from the write-behind policy.

Overall, *aIOLi* is still better than any other system in most cases. In general, it is comparable to POSIX-ref and clearly much better than both POSIX and MPI I/O.

2) Several Applications: In this part, experiments focus on the mutual hindering generated by several applications executing concurrently. These experiments are composed of two parts: firstly a parallel I/O intensive application is started simultaneously with a non I/O intensive program. In the second part two similar I/O intensive applications compete for access to the storage. This test aims at demonstrating the balancing capabilities of *aIOLi*: no application is inferior to another one and all benefit from *aIOLi* optimisations. The completion time is measured as it is more relevant when applications are different.

a) Impact of an I/O Intensive Application on a Less Intensive One: The parallel I/O intensive application is simulated by one IOR instance deployed on 32 single processor nodes. The file size has been set to 4GB and the file access granularity ranges from 8KB to 4MB. For this application, in the first case, POSIX is used for I/O calls and in the second one, MPI I/O is exploited.

The non I/O intensive program is a cat-like program retrieving sequentially a 16MB file from a single node using granularities ranging from 8KB to 512KB. The latter application is small (runs alone, it completes in less than 1s) and is started during the execution of the first one (15s after its beginning). For this application, POSIX is always used for accesses. As all the results lead to the same conclusion, we only present tests using a cat granularity of 32KB4. Due to the immense difference between the completion times, a log scale for the Y axis has been used. Figure 7 presents the results of this test. The solid lines correspond to the IOR results and the dashed lines are for the cat-like operations.

4other results can be found on the *aIOLi* website.

![7_image_1.png](7_image_1.png)

As we can see, *aIOLi* improves the efficiency for both applications. For smaller granularities and due to the adaptive window, the IOR application benefits more than the cat program. Indeed, as we mentioned in the former section, aIOLi optimises sequential accesses when large granularities are exploited.

Concerning MPI I/O, *aIOLi* (curve IOR-aIOLi) provides better performance for the IOR benchmark (curve IOR-aIOLi vs IOR-MPI I/O). However, as we expected, the MPI I/O
routines reduce congestion issues on the NFS server side which leads to better performance than the POSIX API for the cat program (curve cat-MPI I/O vs cat-POSIX) and even than aIOLi for small granularities. The synchronisation mecanisms exploited in MPI I/O routines enable cat requests to be proceed by the server.

b) Impact of Two I/O Intensive Applications: In this second part, we analyse the behaviour of two concurrent IOR instances. Each instance is made of 32 process deployed on 32 nodes.

As expected in figure 8, the execution of two I/O intensive applications degrades performance for all instances, but *aIOLi* minimizes this phenomenon while balancing the I/O accesses between them. Even, if the collective MPI I/O performance

![7_image_2.png](7_image_2.png)

shows a slight improvement for small granularities, it quickly reaches the POSIX one for granularities greater than 128KB.

Moreover, even if the MPI I/O collective approach is improved by *aIOLi* (aIOLi curves on the right), the best result is provided by the standard API POSIX under *aIOLi*.

aIOLi takes advantage of all the freedom given by POSIX
without synchronisation overhead, this is why it performs better than MPI I/O. For the smallest granularity, POSIX requires more than 1 hour and half, MPI I/O needs 11 minutes and 30 seconds whereas *aIOLi* only takes 2 minutes and 35 seconds.

Regarding the fairness, 2 IOR benchmarks have been launched at the same time. The graph does not enable to notice (curves for same API are overlapped) but at worst, the gap between the two completion times is 8.5 seconds for 8KB
but with a 35 times improvement for POSIX and near 5 for MPI I/O. By choosing more specific quantum value for each accessed file, it is possible to set up a desired quality of service for each parallel application running on the cluster. In our case, the quantum variations are similar for both IOR benchmarks.

c) High Concurrency: In this last part, the completion time of 10 distinct applications launched under a NFS server and a NFS server plugged to aIOLi are discussed. 96 nodes have been dedicated for this experiment. The table II summarizes all values. The description of each application is given:
4 applications have worked on the file in a parallel way and 6 others in sequential. The whole size of data represents 6GB.

On a traditional NFS server, the MPI I/O mechanisms exploited inside the parallel I/O intensive applications favor the sequential program: the optimisations are done on the client side which tends to reduce congestion issue on the NFS and thus enables to proceed the smaller tasks. Unfortunately, these optimisations are not well suited for a multi-applicative environment and the overhead generated by internal MPI I/O
mechanisms becomes important. Regarding the NFS server plugged to aIOLi, both POSIX and MPI I/O performances are significantly improved for all the applications. The execution of all applications requires less than 2 minutes and 23 seconds for aIOLi whereas POSIX needs closed to 10 minutes and MPI I/O takes 14 minutes. Finally, once again, the optimisations made by MPI I/O for parallel applications favor on the one hand the sequential programs but on the other hand add useless overhead on parallel I/O intensive programs.

## V. Future Work

During the performance evaluation of our extended NFS
server, we have found an unusual behavior due to the file system protocol granularity and file system implementation. When several processes are deployed on the same node, they strive to access the NFS client layer which results in some starvation problems between them. To solve this problem, we need to force the file system client part to proceed the request in the order we choose. It can be done by issuing only one request at a time. In that case, even if requests are divided into smaller ones, they are still issued in the aggregation order.

aIOLi already has the required internal structure to support this

| Completion time                                           |         |           |         |      |
|-----------------------------------------------------------|---------|-----------|---------|------|
| Application description                                   | NFS     | NFS+aIOLi |         |      |
| POSIX                                                     | MPI I/O | POSIX     | MPI I/O |      |
| Parallel file decomposition read: 2GB, 32 nodes (128KB)   | 490     | 840       | 134     | 500  |
| write: 2GB, 32 nodes (128KB)                              | 409     | 815       | 107     | 604  |
| read: 256MB, 16 nodes (8KB)                               | 595.5   | 728       | 104     | 415  |
| write: 128MB, 8 nodes (64KB)                              | 51      | 257       | 14.5    | 247  |
| Sequential write from/read on 1 node read: 32MB (4KB) 531 | 9       | 48.5      | 3       |      |
| write: 32MB (4KB)                                         | 208     | 9         | 47      | 6    |
| read: 4MB (32KB)                                          | 57      | 1.5       | 6       | 1    |
| write: 4MB (32KB)                                         | 39      | 2         | 19      | 2    |
| read: 1GB (2MB)                                           | 558     | 59        | 143.5   | 54   |
| write: 512MB (2MB)                                        | 192     | 71        | 84      | 61.5 |
| TABLE II                                                  |         |           |         |      |

File access granularity are given between parentheses. Completion time values are given in second.

mechanism. We just need to insert it into the Linux Virtual File System on the client side and to launch another instance of *aIOLi* on the server side to make it work. Moreover, we observed that congestion issue on the server side is reduced by the MPI I/O optimisations made on the client side. An approach made of two aIOLi modules may also moderate the load of the file server.

This kind of approach will end up in a multi-level scheduler.

On the local node, *aIOLi* chooses the best requests order according to the knowledge of all pending I/O operations generated by local processes. On the server side, *aIOLi* schedules requests considering the global traffic coming from all the nodes. Finally, an I/O scheduler in the Linux operating system chooses the most suitable requests order according to the layout of the data on the physical storage medium.

Eventually, each I/O operation is handled by many consecutive schedulers working at different levels and optimising at different access granularity. We call this scheduling method:
cascading scheduling. A similar idea consists of exploiting the meta node concept used by several modern file systems (GPFS,
NFSV4, Lustre, etc.) to provide consistency on files. Each time a process accesses a file for the first time, it becomes the meta node for this file and will be in charge of the coherency for this file. Our suggestion is to add the *aIOLi* scheduler at the same level. In this way, the meta node will schedule I/O requests for this file. The *cascading scheduling* is thus deployed and well balanced on several nodes on the cluster which reduces congestion issues.

## Vi. Conclusion

In this article, we have presented *aIOLi* , a framework for high performance file access in distributed multi-applications environments. We emphasized all the original characteristics that make the strength of *aIOLi* : it is transparent to the applications because it is placed between them and the underlying I/O subsystems, it maximizes the file system throughput by taking advantage of aggregation opportunities despite the distributed context, it maintains fairness between applications by using a quantum-based scheduling algorithm and it does not degrade interactive tasks behavior by deriving the scheduling algorithm from the MLF algorithm.

We validated *aIOLi* by conducting experiments: first on widespread Bonnie++ and b_eff_io to observe overhead and scalability and then on the IOR benchmark and cat-like programs to evaluate efficiency. Results show that our approach improves performance dramatically, when several distributed applications make simultaneous access to distinct files. Furthermore, as expected, our solution maintains fairness between applications and does not degrade interactivity significantly.

We plan to extend *aIOLi* to enable its cooperation with parallel file systems, by taking the distributed data layout into account. We are currently studying how *aIOLi* could be connected to a parallel version of NFS and to Lustre.

## References

[1] A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. K. Hollingsworth, J. Saltz, and A. Sussman. Tuning the performance of I/O intensive parallel applications. In *Proceedings of* the Fourth Workshop on Input/Output in Parallel and Distributed Systems, pages 15â€“27, Philadelphia, May 1996. ACM Press.

[2] N. Bansal. *Algorithms for Flow Time Scheduling*. PhD thesis, Dept. of Computer Science, Carnegie Mellon University, 2003.

[3] P. Cao, E. W. Felten, A. R. Karlin, and K. Li. Implementation and performance of integrated application-controlled file caching, prefetching, and disk scheduling. *ACM Transactions* on Computer Systems, 14(4):311â€“343, 1996.

[4] P. H. Carns, W. B. Ligon III, R. B. Ross, and R. Thakur. PVFS:
A parallel file system for linux clusters. In Proceedings of the 4th Annual Linux Showcase and Conference, pages 317â€“327, Atlanta, GA, October 2000. USENIX Association.

[5] F. Chen and S. Majumdar. Performance of parallel i/o scheduling strategies on a network of workstations. *Eighth International* Conference on Parallel and Distributed Systems, 2001.

[6] P. E. Crandall, R. A. Aydt, A. A. Chien, and D. A. Reed.

Input/output characteristics of scalable parallel applications. In Proceedings of Supercomputing '95, San Diego, CA, December 1995. IEEE Computer Society Press.

[7] K. A. S. Elizabeth Shriver, Christopher Small. Why does file system prefetching work? In *Proceedings of the USENIX Annual* Technical Conference - Monterey, California, USA, pages 71â€“84, June 1999.

[8] D. Ellard and M. Seltzer. Nfs tricks and benchmarking traps. In Proceedings of the FREENIX *2003 Technical Conference*, pages 101â€“114, San Antonio, TX, June 2003.

[9] D. G. Feitelson, L. Rudolph, and U. Schwiegelshohn. Parallel job scheduling - a status report. In D. G. Feitelson, L. Rudolph, and U. Schwiegelshohn, editors, *Job Scheduling Strategies for* Parallel Processing, pages 1â€“16. Springer Verlag, 2004. Lect. Notes Comput. Sci. vol. 3277.

[10] F. Isaila, G. Malpohl, V. Olaru, G. Szeder, and W. Tichy.

Integrating collective I/O and cooperative caching into the
"clusterfile" parallel file system. In *Proceedings of the 18th* Annual International Conference on Supercomputing, pages 58â€“ 67, Sain-Malo, France, July 2004. ACM Press.

[11] R. Jain, K. Somalwar, J. Werth, and J. C. Browne. Heuristics for scheduling I/O operations. *IEEE Transactions on Parallel* and Distributed Systems, 8(3):310â€“320, March 1997.

[12] D. Kotz. Disk-directed I/O for MIMD multiprocessors. In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, pages 61â€“74. USENIX Association, November 1994. Updated as Dartmouth TR PCS-TR94-226 on November 8, 1994.

[13] A. Lebre and Y. Denneulin. aioli: An input/output library for cluster of smp. In Proceeding of the 5th International Symposium on Cluster Computing and Grid,Cardiff, UK, May 2005.

[14] A. Lebre, Y. Denneulin, and T.-T. Van. Controlling and scheduling parallel i/o in a multi-applications environment. Technical Report Research Report RR5689, INRIA, 38330 Montbonnot FR, September 2005.

http://aioli.imag.fr/DOWNLOADS/aiolimaster-reportrr5689.pdf.

[15] X. Ma, M. Winslett, J. Lee, and S. Yu. Improving MPI IO output performance with active buffering plus threads. In Proceedings of the International Parallel and Distributed Processing Symposium. IEEE Computer Society Press, April 2003.

[16] K. Magoutis, S. Addetia, A. Fedorova, M. Seltzer, J. Chase, A. Gallatin, R. Kisley, R. Wickremesinghe, and E. Gabber. Structure and performance of the direct access file system, 2001.

[17] N. Nieuwejaar, D. Kotz, A. Purakayastha, C. S. Ellis, and M. Best. File-access characteristics of parallel scientific workloads. *IEEE Transactions on Parallel and Distributed Systems*,
7(10):1075â€“1089, October 1996.

[18] K. Pruhs, J. Sgall, and E. Torng. Online scheduling. In Hanbook of Scheduling, chapter 15. CRC Press, 2004.

[19] R. Rabenseifner, A. E. Koniges, J.-P. Prost, and R. Hedges. The parallel effective i/o bandwidth benchmark: b_eff_io.

[20] R. Ross. Reactive scheduling for parallel i/o systems, 2000. [21] R. L. F. B. Schmuck. Gpfs: A shared-disk file system for large computing clusters. In Proceedings of the 5th Conference on File and Storage Technologies, January 2002.

[22] P. Schwan. Lustre : Building a file system for 1,000-node clusters. In *Proceedings of the Linux Symposium, Ottawa*, July 2003.

[23] K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett.

Server-directed collective I/O in Panda. In Proceedings of Supercomputing '95, San Diego, CA, December 1995. IEEE Computer Society Press.

[24] M. Seltzer, P. Chen, and J. Ousterhout. Disk scheduling revisited. In *Proceedings of USENIX*, pages 313â€“323, 1990.

[25] R. Thakur, W. Gropp, and E. Lusk. Data sieving and collective I/O in ROMIO. In Proceedings of the Seventh Symposium on the Frontiers of Massively Parallel Computation, pages 182â€“189.

IEEE Computer Society Press, February 1999.

[26] M. Vilayannur, A. Sivasubramaniam, M. Kandemir, R. Thakur, and R. Ross. Discretionary caching for i/o on clusters. May 2003.

[27] A. C. L. W. E. R. W.K. Liao, K. Coloma and S. Tideman.

Collective caching: Application-aware client-side file caching. In *Proceedings of the 14th IEEE International Symposium on* High Performance Distributed Computing, July 2005.