# Transparent I/O-Aware GPU Virtualization for Efficient Resource Consolidation

Nelson Mimura Gonzalez, Tonia Elengikal *IBM Thomas J. Watson Research Center* 1101 Kitchawan Road, Yorktown Heights, New York, United States {nelson, tonia.elengikal}@ibm.com

*Abstract*—Graphics processing units (GPUs) are widely used in high performance computing (HPC) and cloud computing to accelerate workloads. Virtualization provides flexible access to resources while improving utilization and throughput. This is essential to resource disaggregation, which allows ubiquitous access to remote resources among nodes. However, remote GPU virtualization at scale suffers from severe performance degradation due to inter-node communication and resource consolidation overhead, especially for data-intensive workloads.

We propose HFGPU, a GPU virtualization solution transparent to application code based on application programming interface (API) remoting. We define a virtual device manager that allows remote GPUs to be seen, managed, and used as though they were local. To perform at scale we combine multi-adapter InfiniBand networking with a novel distributed I/O forwarding mechanism that eliminates consolidation bottlenecks and reduces data movement. Experiments with up to 1024 NVIDIA V100 GPUs demonstrate overhead lower than 1% for data-intensive operations.

*Index Terms*—GPU, virtualization, consolidation, disaggregation, cloud, HPC, HFGPU, OmniGPU, HFCUDA

#### I. INTRODUCTION

Virtualization allows GPUs to be transparently shared among several applications [1], leveraging heterogeneous compute nodes in a flexible and elastic way [2]. In an HPC cluster this can be used to improve resource utilization and throughput by allocating GPUs that would otherwise be idle [3]. In a cloud platform this provides scalable access to accelerators [4], reducing the total cost of ownership for customers.

Remote GPU virtualization is a particular case that targets remote devices [5], allowing compute nodes to see and use more GPUs than the ones locally available [3]. Inter-node communication becomes critical to deliver performance at scale [6], [7]. However, there is a growing disparity between CPU-GPU bandwidth and network bandwidth. On Summit, a leadership-class system [8] and one of the fastest supercomputers in the world [9], each node is powered by six NVIDIA V100 GPUs with NVLink for high-speed CPU-GPU communication [10] and two EDR InfiniBand adapters for networking. This leads to a bandwidth gap of 300 GB/s of CPU-GPU versus 25 GB/s of network, a 12x slowdown observed when feeding virtualized GPUs from a remote node.

In a conventional scenario, a certain number of application processes must run on each node to use collocated GPUs. With virtualization, remote GPUs are reachable without needing a dedicated application process on each node. This leads to resource consolidation [11], concentrating these application processes in fewer nodes. However, this widens the bandwidth gap even more. Using the same example of the Summit node, if we consolidate processes from four nodes into one, now this node must control and interact with 24 remote GPUs using the same two EDR adapters — increasing the gap to 48x,

This paper presents HFGPU, a Heterogeneous Framework computing GPU virtualization solution based on API remoting [12]. A client-server architecture based on an automatically generated set of wrapper libraries implements interception, forwarding, and remote processing of GPU calls. Client nodes initiate workload execution and send requests to server nodes, which handle remote GPU calls. A virtual device management module provides ubiquitous access to GPUs from any node in a cluster, as long as it is accessible over the network. A communication module supports multiple InfiniBand adapters to use all available network bandwidth. To address the bandwidth gap due to resource consolidation and to perform at scale we propose a distributed I/O forwarding mechanism that allows remote resources to directly access data from a distributed file system. This decentralizes data management and handling from a small set of client nodes and distributes the work among all server nodes, directly influencing performance by reducing the amount of traffic being funneled by the application processes.

This paper has two main contributions. First, the I/O forwarding mechanism that effectively bridges the gap between CPU-GPU and network bandwidth and enables the execution of data-intensive workloads using remote (virtualized) GPUs. Second, the performance evaluation at scale, up to 1024 GPUs on 256 nodes, analyzing the overhead generated by the virtualization machinery (measured to be under 1%) and the network degradation. To the best of our knowledge, this paper is the first to address the resource consolidation issue related to the bandwidth gap, and the first to evaluate a GPU virtualization solution at such scale. All relevant code is being made available at *https:// github.com/ IBM/ hf* .

This paper is organized as follows. Section II presents background. Section III summarizes engineering details. Section IV evaluates virtualization overhead and scaling. Section V discusses the I/O forwarding technique. Section VI examines existing work. Section VII concludes and suggests future work.

2

## II. BACKGROUND

## *A. Transparent API Remoting*

We identify three techniques to implement GPU virtualization: *API remoting*, based on a wrapper library that intercepts GPU calls [12]; *device virtualization*, based on driver-level emulation [13], [14]; and *hardware supported*, based on direct pass-through GPU access [15]. Table I presents description, pros, and cons of each technique. We based the design and implementation of HFGPU on API remoting because, in our view, it provides virtualization at a higher level that overcomes the black-box limitation of GPU drivers [2].

Figure 1 illustrates the architecture of an API remoting solution composed by: a *client* part, that intercepts and forwards GPU calls executed by the application; and a *server* part, that receives incoming calls, executes them using local resources, and returns results to the application.

![](_page_1_Figure_4.png)

Fig. 1. API remoting client-server architecture

Figure 2 illustrates the *call forwarding* mechanism using as example a GPU memory allocation function, alloc. This function receives the size of the allocation and returns the address of the allocated region. The interception mechanism is a wrapper library that reimplements the GPU API by overriding its functions [2]. The application can be linked against the wrapper library during compilation or it can be dynamically loaded during execution, for instance via LD_PRELOAD mechanism [16]. When the application calls alloc, the client intercepts and forwards the call with the size argument to the server. The server receives the call, executes the original alloc function using its local GPUs, and returns the resulting address to the application. Call forwarding and remote execution are completely transparent from application perspective. Note that the data used in any call, including CPU-GPU memory transfers, now must be exchanged between nodes over the network.

![](_page_1_Figure_7.png)

Fig. 2. Example of call forwarding

#### *B. Bandwidth gap and resource consolidation*

Figure 3 shows simplified node diagrams of three systems designed for HPC at large scale: the S822LC 8335-GTA (code name *Firestone*) [17], the S822LC 8335-GTB (code name *Minsky*) [18], and the AC922 8335-GTW (code name *Witherspoon*) [10]. Table II summarizes the CPU-GPU and network bandwidth values for each of these systems. We calculate the aggregate bandwidth for GPUs and network cards by multiplying the number of devices by the bandwidth per device. The *bandwidth gap* between CPU-GPU and network increased from 2.56x to 12.00x in a short span of four years, a result of the rapid evolution and adoption of CPU-GPU bus technologies. While this gap is not a problem in a scenario without virtualization, with remote GPUs the memory transfers become network transfers, therefore putting pressure on the communication layer.

|  |  | CPU-GPU VERSUS NETWORK BANDWIDTH |  |  |
| --- | --- | --- | --- | --- |
| System | Year | CPU-GPU | Network | Ratio |
| Firestone | 2015 | 32.0 GB/s | 12.5 GB/s | 2.56x |
| Minsky | 2016 | 80.0 GB/s | 25.0 GB/s | 3.20x |
| Witherspoon | 2018 | 300.0 GB/s | 25.0 GB/s | 12.00x |

TABLE II

The evolution of GPU *compute power* is another challenge to GPU virtualization. An evaluation using three different generations of GPUs [1] concluded that the virtualization overhead for newer models was 8 to 14 times higher than older models. Since the newer generation was able to compute faster, kernel execution time decreased and therefore other time components, such as data transfers, became more significant. This puts even more pressure on the communication layer, which in this case typically becomes the bottleneck.

TABLE I SUMMARY OF GPU VIRTUALIZATION TECHNIQUES

| Technique | Description | Pros | Cons |
| --- | --- | --- | --- |
| API | Wrapper library with the same API | Negligible overhead (simple virtual | Must keep track of API changes; |
| Remoting | of the original library intercepts and | ization architecture); no reverse engi | no virtualization features (e.g., live |
|  | forwards calls to virtualized GPUs. | neering of GPUs at driver level. | migration, fault tolerance). |
| Device | Virtualization with custom driver for | No changes to application layer; uses | Relies on knowledge of typically pro |
| Virtualization | specific operations (paravirt.) or us | existing GPU libraries and ready for | prietary drivers, requiring a continu |
|  | ing original drivers (full virt.). | changes in those libraries. | ous reverse engineering effort. |
| Hardware | Direct pass-through using hardware | No extra software layer (near-native | Difficult to impose GPU scheduling |
| Supported | extension features. | performance). | policies (no interaction with OS). |

![](_page_2_Figure_0.png)

Fig. 3. Simplified node diagrams of three generations of high-performance systems

The main challenge to GPU virtualization is *consolidation*, which refers to concentrating application processes in fewer nodes. Figure 4 shows the progression from a local (conventional) to a consolidated scenario with virtualization.

- *Local scenario*: Figure 4a illustrates two nodes (horizontal rectangles), each composed by one CPU with 16 cores (blocks on the left) and 4 GPUs (blocks on the right). Some form of parallelization (e.g., MPI) is used to execute an application distributed in several processes, each process controlling one GPU. Dark blocks indicate resources being used; light blocks indicate idle resources. In this example, each process runs on its own CPU core, therefore we use 4 CPU cores to control the 4 local GPUs. We must run 4 processes on each node since GPUs can only be used from collocated CPUs. On each node, CPU-GPU communication is handled by a high-speed bus such as PCIe or NVLink.
- *Virtualization scenario*: Figure 4b illustrates two nodes: one client, which executes the application; and one server, which receives and executes remote GPU calls. Now CPU-GPU communication must go over the network, since the CPUs running the application and the GPUs used in the computation are in different physical nodes.

![](_page_2_Figure_5.png)

Fig. 4. Setup progression from local scenario to disaggregation

- *Consolidation scenario*: Figure 4c illustrates five nodes: one client and four servers. In a conventional scenario, each node would require 4 processes to control their local GPUs. With virtualization it is possible to control all 16 GPUs from a single node. CPU resources from the server nodes become free while CPU utilization in the client node is improved, since all cores are being actively used.
- *Disaggregation scenario*: Figure 4d illustrates the next step in this progression, which is making use of the CPU resources freed after consolidating processes in the client node. Disaggregation refers to allowing heterogeneous resources to be freely managed and allocated for different workloads and users. With judicious use of compute and networking resources by combining appropriate types of workloads, it is possible to improve resource utilization while maintaining acceptable performance levels.

Improving resource utilization, however, comes at a cost. Consolidating processes into fewer nodes increases the gap between aggregate CPU-GPU bandwidth and aggregate network bandwidth. Based on numbers similar to the Witherspoon system in Table II the gap for Figure 4b is 16x (4 GPUs to one node); and 64x for Figure 4c (16 GPUs to one node). The more processes consolidated into fewer nodes, the worse the gap becomes since the same network bandwidth is shared to control more remote resources [16].

## III. ENGINEERING

The novelty of HFGPU revolves around the I/O forwarding mechanism and its utilization to improve consolidation and disaggregation scenarios. However, there are several engineering details that, albeit typically outside the scope of a scientific paper, are fundamental to the performance and usability of the solution. The next subsections summarize these details.

#### *A. Automatic Wrapper Generation*

The call forwarding mechanism presented in Section II-A requires function wrappers for each relevant API call. In general, porting functions to HFGPU relies on defining input arguments (sent to server) and outputs (received). Based on this, HFGPU provides a wrapper generator that receives function prototypes and a set of flags indicating inputs, outputs, and if the parameter is a variable or a pointer to a variable, in which case it is necessary to exchange a chunk of memory. Server errors are handled and reported back to the client.

## *B. Kernel Execution Support*

Using NVIDIA CUDA library as example, up to version 9.1 the library defined functions to configure the call, cudaConfigureCall; set up function arguments, cudaSetupArgument; and and launch the function, cudaLaunch. HFGPU relied on the dladdr, dlopen, and dlsym calls to retrieve the kernel name, load the kernel function on the server node, and execute it using remote GPUs.

Starting from CUDA 9.2, the API changed to a single call, cudaLaunchKernel, that operates on an opaque list of parameters. This required reverse engineering of the ELF file using Elf64_Ehdr and Elf64_Shdr structures to navigate the program binary. Function cudaLaunchKernel triggers a call to cuModuleLoadData to load a module from an image. HFGPU runs an ELF parsing routine that assigns the image address to an Elf64_Ehdr variable, then iterates over its .nv.info sections. These sections specify kernels properties, including number of arguments and sizes. HFGPU parses this information and builds a table of functions. Each entry contains a name and a list arguments with sizes. The kernel launch process calls cuModuleGetFunction to retrieve a function pointer based on its name. HFGPU client intercepts name, uses it to retrieve kernel information from the table, and ships the function to the server. This solution is completely transparent to application code.

#### *C. Virtual Device Management*

Figure 5 shows two scenarios, local and virtualized, each representing four nodes (A to D) with four GPUs each. In a node with N devices, CUDA numbers GPUs from 0 to N-1. Each host thread has one active device that is targeted by GPU functions. The program can call cudaSetDevice to change the active device for subsequent calls. Function cudaGetDeviceCount returns N. HFGPU receives a list of host:index pairs that determines the GPUs visible to the program. The variable is processed before program's main via GCC's constructor property. Figure 5 shows the string used to create the virtualized scenario. String indices are the ones locally assigned by CUDA. Once processed, HFGPU generates virtual indices as depicted on the right. For instance, device 0 from node C becomes virtual device 3. Then, device management wrappers implement the logic for the program to see virtual devices as though they were local. For instance, calling cudaGetDeviceCount will return 8.

![](_page_3_Figure_5.png)

Fig. 5. Virtual device management

## *D. Memory Management*

Memory transfer functions require four parameters: a destination address, dst; a source address, src; the data size, count; and the direction, kind. The value of kind determines if src and dst point to CPU and/or GPU memory. A buffer is allocated and used by the server to stage data to and from the GPU. This buffer is pre-allocated (e.g., during server initialization) using pinned memory to improve latency and bandwidth. HFGPU keeps a table of memory allocations to know if a pointer passed to a kernel refers to CPU or GPU data.

#### *E. Multi-Adapter InfiniBand Support*

Communication is a key aspect of HFGPU. A first version of the networking layer used rsocket, a library built on top of libibverbs and librdmacm that enables communication over InfiniBand. A second version used MPI, which provides a simple programming interface and implements several algorithms and strategies to optimize performance. HFGPU takes care of proper MPI initialization and handles its global communicator, MPI_COMM_WORLD. Since HFGPU requires extra processes to behave as servers, HFGPU determines the number of server processes and uses MPI_Comm_split to separate client and server processes. This generates a communicator that is assigned to a global variable. Since there is no trivial way to change MPI_COMM_WORLD, we opted for providing function wrappers for MPI calls that receive a communicator as argument. Whenever a call references MPI_COMM_WORLD, we replace it by the previously assigned global variable.

HFGPU implements two strategies to use multiple Infini-Band adapters: *striping*, allowing one thread to use all available adapters; and *pinning*, improving the aggregate bandwidth for one or more processes. Due to NUMA effects, transferring data from a network interface connected to one CPU to a GPU connected to a different CPU might degrade overall performance, since it requires CPU to CPU communication. The pinned strategy typically renders better performance since it minimizes CPU to CPU communication: adapter(s) connected to a CPU interact with GPU(s) also connected to that CPU.

#### IV. VIRTUALIZATION OVERHEAD AND SCALING

HFGPU virtualization overhead has two components:

- the machinery cost of routing GPU calls through HFGPU software components; and
- the network cost of interacting with remote resources.

The machinery cost can be measured by comparing (i) performance of local GPUs (Figure 4a) to (ii) performance of local GPUs (also Figure 4a) with HFGPU. This experiment is limited to a single node to factor out the effects of network degradation. Dividing (ii) by (i) results in a performance factor between 0.0 and 1.0 that indicates the cost of using HFGPU. A number close to 1.0 represents low cost.

The network cost can be measured by comparing (i) to (iii) performance of remote GPUs with HFGPU (Figure 4b) executed with one or mode nodes. Dividing (iii) by (i) for the same number of nodes results in a performance factor between 0.0 and 1.0 that indicates the cost of using HFGPU over the network. Again, a number close to 1.0 represents low cost. We can also calculate more traditional metrics, such as speedup, the performance improvement factor after increasing the amount of resources dedicated to execute a program; and parallel efficiency, the ratio between the speedup and the resource increase factor.

We ran experiments on a cluster of Witherspoon nodes (see Figure 3). Each node has two IBM POWER9 CPUs (a total of 44 cores), 512 GB of DDR4 memory, and six NVIDIA Tesla V100 GPUs with 16 GB memory each. Nodes are interconnected by two EDR InfiniBand 100 Gb/s links in a regular switched network. The nodes run RHEL Server release 7.5 (Maipo). We executed up to 32 client (MPI) processes on each client node. Results report the average of five or more repetitions per experiment. We evaluated overhead and scaling using four workloads. In all our experiments the *machinery cost was lower than 1%*. The next subsections discuss the overhead and scaling results for each workload.

## *A. DGEMM*

General matrix multiplication workload using double precision floating point values. The implementation is based on the cuBLAS library. The experiments used 2 GB matrices. The results are presented in Figure 6. Some initial remarks:

- In all charts the X-axis represents the number of GPUs in logarithmic scale (base 2).
- In the top left chart, Time, the Y-axis represents the elapsed time (in seconds) to execute the experiment.
- In the top right chart, *Speedup*, the Y-axis represents the speedup value calculated by dividing the time spent on

the experiment with one GPU by the time spent on the experiment with N GPUs.

- In the bottom left chart, *Parallel Efficiency*, the Y-axis represents the efficiency factor calculated by dividing the speedup by the resource increase factor to the experiment with one GPU. This indicates if the utilization of more resources is being translated into more performance.
- In the bottom right chart, *Performance Factor*, the Yaxis represents the factor calculated by dividing the elapsed time for HFGPU by the elapsed time for local GPUs (without virtualization). This generates a number between 0.0 and 1.0. A number close to 1.0 means that virtualized performance is close to local (non-virtualized) performance. A low number means that degradation factors such as networking bottlenecks lead to significant performance loss.

The charts reveal that both local and HFGPU experiments show good scaling properties. The HFGPU scenario shows a slight performance degradation when increasing number of nodes, with the performance factor starting at 0.96 for 1 node and staying around 0.90 up to 64 nodes. DGEMM is a compute-intensive workload. We executed DGEMM using the largest matrices we could fit in the GPUs. The larger the matrices, the higher the computational demand compared to the cost of transferring data to the GPU. Since CPU-GPU data transfers become network operations, this property is key to hide the data movement costs associated with GPU virtualization.

64

![](_page_4_Figure_11.png)

![](_page_4_Figure_12.png)

Fig. 6. DGEMM performance

Fig. 7. DAXPY performance

8.00

## *B. DAXPY*

Scaled vector addition using double precision floating point values. DAXPY is the complete opposite of DGEMM in terms of resource utilization. DAXPY is data-intensive, meaning that the computational requirement grows much slower than its demand for data bandwidth. In fact, DAXPY is a bad candidate for GPUs at all — virtualized or not — since the compute acceleration factor does not amortize the time required to move data in and out of the GPU. It is faster to run it on the CPU.

These properties are observed in Figure 7. The first step, from one to two GPUs, has a parallel efficiency of 70% for local and 79% for HFGPU. DAXPY is the only workload we tested in which the performance factor actually increases not because the HFGPU performance improves, but because the local performance quickly degrades. DAXPY is a great (and quite possibly extreme) example of bad candidate to run on remote GPUs; a data-intensive workload that simply does not have enough computational requirement to hide the data movement costs, amplified by the fact of being remote.

## *C. Nekbone*

Nekbone represents part of Nek5000, a high-order incompressible Navier-Stokes solver used in thermal hydraulics. Nekbone is an MPI code written in C and Fortran. The code is computationally intense and the communication is represented by nearest-neighbor data exchanges and vector reductions. The results of the experiments are presented in Figure 8. Different from DGEMM and DAXPY, instead of *Time* we use a *Figure of Merit* proportional to the computational capacity achieved — the higher, the better. This also modifies how speedup is calculated — by dividing the parallel FOM by the FOM for one GPU; and how the performance factor is calculated — by dividing HFGPU FOM by local FOM.

We executed Nekbone using 1 to 1024 GPUs, 4 GPUs per node. Performance for local GPUs scales almost perfectly. For 1024 GPUs the results show a performance factor of 0.85 comparing HFGPU to local. HFGPU parallel efficiency starts at 100% for 2 nodes and stays above 90% up to 512 GPUs. Parallel efficiency at 1024 GPUs is 97% for local and 85% for HFGPU. The performance factor is higher than 0.90 up to 128 GPUs. Nekbone is a good candidate for using virtualized GPUs due to its computational requirements. The performance factor comparing HFGPU to local is higher than 0.90 up to 128 GPUs, and stays above 0.85 up to 1024 GPUs.

## *D. AMG*

Parallel algebraic multigrid solver for linear systems. The code is written in C, uses MPI and OpenMP, and is highly synchronous and memory-access bound. We measured a local virtualization overhead lower than 1%. Results comparing multiple local and remote GPUs are presented in Figure 9. Similar to Nekbone, performance is measured and compared using a FOM (Figure of Merit). Due to frequent and intensive data movement, AMG performance quickly degrades when increasing the number of GPUs for the virtualized scenario. HFGPU parallel efficiency starts at 96% for 2 nodes, it decreases to about 80% for 32 nodes, then it quickly degrades to 59% for 256 nodes and 43% for 1024 nodes. The inability to hide data movement cost also is evident by observing the performance factor between HFGPU and local. The performance factor starts at 0.98 for 1 node, slowly degrades to 0.81 for 64 nodes, and then quickly degrades to 0.53 for 1024 nodes.

![](_page_5_Figure_9.png)

Fig. 8. Nekbone performance

Fig. 9. AMG performance

Similar to DAXPY, data intensive operations combined to a large number of GPUs lead to a severe bandwidth gap which translates into performance deterioration.

## V. I/O FORWARDING

Guaranteeing low overhead is key to have a functional solution. However, just good engineering and technology choices regarding the virtualization machinery and the networking layer are not enough to achieve reasonable performance, in particular to data-intensive workloads. This is the (bandwidth) gap addressed by our I/O forwarding method.

Figure 10 depicts three scenarios, each comprising one or two nodes and a distributed file system, FS. The arrows represent data exchanges. Thick arrows indicate exchanges that potentially move large data sets. Thin arrows indicate smaller transfers (e.g., function parameters). For simplification purposes, the examples will focus on the read operation from the distributed file system. We assume that the process is similar for write operations.

![](_page_6_Figure_4.png)

Fig. 10. I/O forwarding feature

The first scenario represents a set of data exchanges in a local setup. Assume that a file from FS has been opened using fopen. A program reads data from this file by calling fread, which copies data from the file into a local CPU memory buffer — arrow (a) — then writes data to the GPU using cudaMemcpy to transfer data from CPU memory to GPU memory. The cudaMemcpy call is represented by arrow (b) and the actual data transfer by arrow (c).

The second scenario represents the same sequence in a virtualized setup. The fread part is similar, except that the memory transfer from CPU to GPU occurs between two different nodes. HFGPU forwards the cudaMemcpy call to the server — arrow (b). The actual data transfer is executed from the client to a buffer in the server node — arrow (c). Finally, the data is copied via a local cudaMemcpy at the server — arrow (d).

![](_page_6_Figure_8.png)

Fig. 11. I/O bottleneck due to consolidation

The main difference between local and HFGPU scenarios is that the cudaMemcpy operation becomes a network operation. If several remote GPUs request data, the client node becomes a bottleneck since it has limited networking resources. This is depicted in virtualized scenario of Figure 11. A highperformance file system is able to serve several concurrent requests, represented by thick lines leaving FS. However, the client node C has a limited inbound capacity, therefore thick lines become thin lines (bandwidth has to be shared). A similar effect occurs when sending data to the server nodes S. Each server node has an inbound capacity higher than the outbound capacity of the client when its bandwidth has to be shared among several requests.

The I/O forwarding feature comprises a set of POSIX-like file I/O calls (prefix ioshp) that can be directly used in application code or preloaded as wrappers to the original file I/O calls. The ioshp_* functions behave as their regular POSIX counterparts when the program is executed without HFGPU. With HFGPU, the execution flow follows the *I/O forwarding* scenario depicted in Figure 10. First, the file is opened using ioshp_fopen, which is executed at the client and forwarded to the server. The file pointer is obtained at the server using a regular fopen call, and then returned to the client. To read data from the file, the client calls ioshp_fread. This call is shipped to the server, represented by arrow (a), and translated into two operations: first, an fread, represented by arrow (b), using the file pointer previously obtained and a local buffer; and second, a cudaMemcpy from local buffer to the GPU, represented by arrow (c). The large data transfer occurs only between the server node (using its buffer) and the file system, without direct interaction of the client node. Since the distributed file system has high bandwidth and each server node can use its full bandwidth to exchange data, the I/O bottleneck is eliminated, as depicted in the I/O forwarding scenario of Figure 11. The client only exchanges control information with the server nodes.

## *A. I/O Benchmark*

We executed experiments using the I/O forwarding feature with four codes. The first is an I/O intensive benchmark with configurable transfer size implemented in MPI using a weak scaling approach. Results are presented in Figure 12. The figure shows experiments with four different transfer sizes (X-axis). All experiments were executed using 192 GPUs. This means, for instance, that for the experiments with 8 GB transfers, each GPU received 8 GB for a total of 1536 GB

![](_page_7_Figure_0.png)

Fig. 12. I/O benchmark performance

of data transferred from the distributed file system to the nodes. Also, for each transfer size we executed three different scenarios: local, without HFGPU; MCP, with HFGPU but without the I/O forwarding feature; and IO, making use of ioshp_* functions. Y-axis represents the runtime for each experiment. The results show that HFGPU overhead with I/O forwarding is lower than 1% (close to local performance); HFGPU without I/O forwarding shows a slowdown of 4x.

#### *B. Nekbone*

We modified the code to read and write data structures from files stored in the distributed file system. Results are presented in Figure 13. The X-axis represents the number of GPUs. Nekbone code implements weak scaling, therefore increasing the number of nodes should not affect read and write times since the number of nodes also is increased. As observed in the chart, local and IO performance are fairly constant. IO performance is again within 1% of local performance and 24x faster than MCP, due to network contention associated to the consolidation of processes in fewer nodes. The I/O forwarding feature was also used to efficiently implement checkpoint/restart, a fault-tolerance technique that allows saving and then restoring the state of an experiment.

![](_page_7_Figure_5.png)

Fig. 13. Nekbone performance with I/O forwarding

![](_page_7_Figure_7.png)

Fig. 14. PENNANT performance with I/O forwarding

#### *C. PENNANT*

Mesh physics mini-app for advanced architecture research. PENNANT implements strong scaling, in the sense that the total amount of data written by the application is 9 GB (fixed). Consequently, increasing the number of processes reduces the amount written by each process. Figure 14 shows the results. Local and IO performance are similar, with an virtualization overhead lower than 1%. This is about 50x faster than the scenario without I/O forwarding, due to high-intensity data movement in a short time span.

#### *D. DGEMM*

We implemented three versions of the cuBLAS-based matrix multiplication code: a) init_bcast, initializing the matrices in memory and broadcasting them to all worker nodes; b) fread_bcast, reading the matrices from a file (and then broadcasting); and c) hfio, using I/O forwarding to distribute the read operation (no broadcasting needed). Figures 15 to 17 present the time distribution for each implementation. In each figure the first row of pie charts represent local experiments and the second row represents experiments with HFGPU. The numbers on top represent the number of nodes. All charts are based on results from experiments with six GPUs per node and square matrices of 16384 elements on each side.

For the init_bcast and fread_bcast implementations the local scenario is dominated by bcast (matrix broadcast) while the HFGPU scenario is dominated first by h2d (host to device transfers). Kernel execution (dgemm) and file reads (fread) times are relatively constant across experiments. For hfio (Figure 17) the time distribution essentially does not change from local to HFGPU. Not having collectives (bcast) executed reduces experimental variability and improves performance.

Comparing performance, init_bcast and fread_bcast use the bcast operation to distribute a copy of the matrix to each participating process. Increasing the number of nodes directly affects overall performance. In contrast, for hfio there are no collectives nor remote data transfers triggered by host to device copies. While HFGPU performance is several times slower for the other

![](_page_8_Figure_0.png)

implementations, using I/O forwarding yields average

VI. RELATED WORK

performance within 2% of local.

Table III compares nine API remoting solutions with HFGPU. Only GVM [23] requires changes to source code; the others are application transparent. rCUDA [22] might or might not require changes, depending on the version used. Only five allow remote virtualization. Three provide InfiniBand support: rCUDA, VOCL [16], and DS-CUDA [24]. Only VOCL might be able to use multiple IB adapters (HCAs) since it uses MPI. Finally, none of the solutions have mechanisms to eliminate bottlenecks due to resource consolidation and contention, which is the I/O forwarding function we propose in HFGPU. In fact, to the best of our knowledge, none of the related works explored the bandwidth gap issue presented in this paper, nor the effects of consolidation and disaggregation.

Previous approaches are restricted to smaller experiments. The largest testbed used with rCUDA comprised 5 nodes and 12 GPUs [26]. The largest testbed among other solutions was DS-CUDA with 64 GPUs [24]. Most rCUDA evaluations use 2 nodes with 1 remote GPU. To the best of our knowledge, our evaluation using 1024 GPUs is largest used to analyze the performance of a GPU virtualization solution.

In addition, previous works have not explored data-intensive benchmarks. The latest rCUDA memory copy evaluation uses copy sizes up to 64 MB [27]. A molecular dynamics code [28] shows data movement and capacity requirement lower than 500 MB – only 5% of 10 GB provided by the two NVIDIA K20m used in the study. Compared to the local scenario, the solution virtualization is 2.0x and 1.5x slower for one and two instances, respectively. The experimental setup forces heavy CPU multiplexing among threads in the local scenario, leading to a 35x slowdown for 4 instances.

## VII. CONCLUSION AND FUTURE WORK

This paper presented HFGPU, a transparent GPU virtualization solution based on API remoting. Achieving reasonable performance required two steps. First, low overhead, which was addressed by careful engineering and implementation of the networking layer and the machinery in general. Making full use of the networking resources available was key to start bridging the bandwidth gap between CPU-GPU and network. The results revealed a machinery overhead (strictly due to having the virtualization layer) lower than 1%. Compute intensive workloads such as DGEMM and Nekbone are good candidates to be executed using remote GPUs since they are able to hide the data movement cost.

| Solution | Application | Local | Remote | InfiniBand | Multi-HCA | I/O |
| --- | --- | --- | --- | --- | --- | --- |
|  | Transparent | Virtualization | Virtualization | Support | Support | Forwarding |
| GViM [19] | Y | Y | N | N | N | N |
| vCUDA [20] | Y | Y | N | N | N | N |
| GVirtuS [21] | Y | Y | Y | N | N | N |
| rCUDA [22] | Y | Y | Y | Y | N | N |
| GVM [23] | N | Y | N | N | N | N |
| VOCL [16] | Y | Y | Y | Y | Y | N |
| DS-CUDA [24] | Y | Y | Y | Y | N | N |
| vmCUDA [25] | Y | Y | N | N | N | N |
| FairGV [5] | Y | Y | Y | N | N | N |
| HFGPU | Y | Y | Y | Y | Y | Y |

TABLE III COMPARISON OF EXISTING API REMOTING SOLUTIONS TO HFGPU

Second, the I/O forwarding feature that effectively decentralizes data movement and makes use of the underlying shared file system to distribute data more efficiently. This step is crucial to achieve good performance, especially for data-intensive workloads; otherwise, the comparatively low computation cost is not enough to amortize the time needed to move data to feed the GPUs. Using the I/O forwarding feature yielded similar performance levels to native (local) resources, showing that thoughtful management of data movement is key to extract performance of a disaggregated setup.

Future work comprises expanding the work on resource disaggregation, in particular the experiments with heterogeneous workloads. Regarding HFGPU, we intend to integrate other technologies such as GPUDirect to improve data transfers from and to GPUs. We can leverage the MPI communication layer to implement collectives within the HFGPU machinery. We also intend to encompass other features, such as Unified Memory, as well as other programming platforms and devices.

#### ACKNOWLEDGMENTS

This work was supported in part by Lawrence Livermore National Laboratory on behalf of the U.S. Department of Energy, under Lawrence Livermore National Laboratory subcontract no. B621073.

## REFERENCES

- [1] C. Reano and F. Silla, "A comparative performance analysis of remote ˜ gpu virtualization over three generations of gpus," in *2017 46th International Conference on Parallel Processing Workshops (ICPPW)*, Aug 2017, pp. 121–128.
- [2] C.-H. Hong, I. Spence, and D. S. Nikolopoulos, "Gpu virtualization and scheduling methods: A comprehensive survey," *ACM Comput. Surv.*, vol. 50, no. 3, pp. 35:1–35:37, Jun. 2017. [Online]. Available: http://doi.acm.org/10.1145/3068281
- [3] F. Silla, J. Prades, S. Iserte, and C. Reano, "Remote gpu virtualization: ˜ Is it useful?" in *2016 2nd IEEE International Workshop on High-Performance Interconnection Networks in the Exascale and Big-Data Era (HiPINEB)*, March 2016, pp. 41–48.
- [4] S. Crago, K. Dunn, P. Eads, L. Hochstein, D. Kang, M. Kang, D. Modium, K. Singh, J. Suh, and J. P. Walters, "Heterogeneous cloud computing," in *2011 IEEE International Conference on Cluster Computing*, Sep. 2011, pp. 378–385.
- [5] C. Hong, I. Spence, and D. S. Nikolopoulos, "Fairgv: Fair and fast gpu virtualization," *IEEE Transactions on Parallel and Distributed Systems*, vol. 28, no. 12, pp. 3472–3485, Dec 2017.
- [6] C. Reano, F. Silla, G. Shainer, and S. Schultz, "Local and remote ˜ gpus perform similar with edr 100g infiniband," in *Proceedings of the Industrial Track of the 16th International Middleware Conference*, ser. Middleware Industry '15. New York, NY, USA: ACM, 2015, pp. 4:1– 4:7. [Online]. Available: http://doi.acm.org/10.1145/2830013.2830015
- [7] C. Reano and F. Silla, "Reducing the performance gap of remote gpu ˜ virtualization with infiniband connect-ib," in *2016 IEEE Symposium on Computers and Communication (ISCC)*, June 2016, pp. 920–925.
- [8] O. R. N. Laboratory. (2019, jan) Summit faqs oak ridge leadership computing facility. [Online]. Available: https://www.olcf. ornl.gov/olcf-resources/compute-systems/summit/summit-faqs/
- [9] Top500. (2018, nov) Top500 supercomputer sites november 2018. [Online]. Available: https://www.top500.org/lists/2018/11/
- [10] A. B. Caldeira, "Ibm power system ac922 introduction and technical overview," IBM, Tech. Rep., 2018.
- [11] P. Padala, K. G. Shin, X. Zhu, M. Uysal, Z. Wang, S. Singhal, A. Merchant, and K. Salem, "Adaptive control of virtualized resources in utility computing environments," in *Proceedings of the 2Nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007*, ser. EuroSys '07. New York, NY, USA: ACM, 2007, pp. 289–302. [Online]. Available: http://doi.acm.org/10.1145/1272996.1273026

- [12] L. Shi, H. Chen, J. Sun, and K. Li, "vcuda: Gpu-accelerated highperformance computing in virtual machines," *IEEE Transactions on Computers*, vol. 61, no. 6, pp. 804–816, June 2012.
- [13] M. Dowty and J. Sugerman, "Gpu virtualization on vmware's hosted i/o architecture," *SIGOPS Oper. Syst. Rev.*, vol. 43, no. 3, pp. 73–82, Jul.
- 2009. [Online]. Available: http://doi.acm.org/10.1145/1618525.1618534 [14] C. I. Dalton, D. Plaquin, W. Weidner, D. Kuhlmann, B. Balacheff, and R. Brown, "Trusted virtual platforms: A key enabler for converged client devices," *SIGOPS Oper. Syst. Rev.*, vol. 43, no. 1, pp. 36–43, Jan. 2009. [Online]. Available: http://doi.acm.org/10.1145/1496909.1496918
- [15] J. P. Walters, A. J. Younge, D. I. Kang, K. T. Yao, M. Kang, S. P. Crago, and G. C. Fox, "Gpu passthrough performance: A comparison of kvm, xen, vmware esxi, and lxc for cuda and opencl applications," in *2014 IEEE 7th International Conference on Cloud Computing*, June 2014, pp. 636–643.
- [16] S. Xiao, P. Balaji, Q. Zhu, R. Thakur, S. Coghlan, H. Lin, G. Wen, J. Hong, and W. Feng, "Vocl: An optimized environment for transparent virtualization of graphics processing units," in *2012 Innovative Parallel Computing (InPar)*, May 2012, pp. 1–12.
- [17] G. S. Alexandre Caldeira, Marc-Eric Kahle and K. C. Vearner, "Ibm power systems s822lc technical overview and introduction," IBM, Tech. Rep., 2015.
- [18] S. V. Alexandre Bicas Caldeira, Volker Haug, "Ibm power system s822lc for high performance computing introduction and technical overview," IBM, Tech. Rep., 2016.
- [19] V. Gupta, A. Gavrilovska, K. Schwan, H. Kharche, N. Tolia, V. Talwar, and P. Ranganathan, "Gvim: Gpu-accelerated virtual machines," in *Proceedings of the 3rd ACM Workshop on System-level Virtualization for High Performance Computing*, ser. HPCVirt '09. New York, NY, USA: ACM, 2009, pp. 17–24. [Online]. Available: http://doi.acm.org/10.1145/1519138.1519141
- [20] L. Shi, H. Chen, and J. Sun, "vcuda: Gpu accelerated high performance computing in virtual machines," in *2009 IEEE International Symposium on Parallel Distributed Processing*, May 2009, pp. 1–11.
- [21] G. Giunta, R. Montella, G. Agrillo, and G. Coviello, "A gpgpu transparent virtualization component for high performance computing clouds," in *Euro-Par 2010 - Parallel Processing*, P. D'Ambra, M. Guarracino, and D. Talia, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010, pp. 379–391.
- [22] J. Duato, A. J. Pena, F. Silla, R. Mayo, and E. S. Quintana-Orti, "Performance of cuda virtualized remote gpus in high performance clusters," in *2011 International Conference on Parallel Processing*, Sep. 2011, pp. 365–374.
- [23] T. Li, V. K. Narayana, E. El-Araby, and T. El-Ghazawi, "Gpu resource sharing and virtualization on high performance computing systems," in *2011 International Conference on Parallel Processing*, Sep. 2011, pp. 733–742.
- [24] M. Oikawa, A. Kawai, K. Nomura, K. Yasuoka, K. Yoshikawa, and T. Narumi, "Ds-cuda: A middleware to use many gpus in the cloud environment," in *2012 SC Companion: High Performance Computing, Networking Storage and Analysis*, Nov 2012, pp. 1207–1214.
- [25] L. Vu, H. Sivaraman, and R. Bidarkar, "Gpu virtualization for high performance general purpose computing on the esx hypervisor," in *Proceedings of the High Performance Computing Symposium*, ser. HPC '14. San Diego, CA, USA: Society for Computer Simulation International, 2014, pp. 2:1–2:8. [Online]. Available: http://dl.acm.org/citation.cfm?id=2663510.2663512
- [26] B. Imbernn, J. Prades, D. Gimnez, J. M. Cecilia, and F. Silla, "Enhancing large-scale docking simulation on heterogeneous systems," *Future Gener. Comput. Syst.*, vol. 79, no. P1, pp. 26–37, Feb. 2018. [Online]. Available: https://doi.org/10.1016/j.future.2017.08.050
- [27] C. Reano, F. Silla, and J. Duato, "Enhancing the rcuda remote gpu ˜ virtualization framework: From a prototype to a production solution," in *Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing*, ser. CCGrid '17. Piscataway, NJ, USA: IEEE Press, 2017, pp. 695–698. [Online]. Available: https://doi.org/10.1109/CCGRID.2017.42
- [28] J. Prades, C. Reano, F. Silla, B. Imbern ˜ on, H. P ´ erez-S ´ anchez, and ´ J. M. Cecilia, "Increasing molecular dynamics simulations throughput by virtualizing remote gpus with rcuda," in *Proceedings of the 47th International Conference on Parallel Processing Companion*, ser. ICPP '18. New York, NY, USA: ACM, 2018, pp. 9:1–9:8. [Online]. Available: http://doi.acm.org/10.1145/3229710.3229734

