# Pioneer: A Solution To Parallel I/O Workload Characterization And Generation

Weiping He University of Minnesota Email: weihe@cs.umn.edu

| University of Minnesota Email: du@cs.umn.edu   |
|------------------------------------------------|

Sai B. Narasimhamurthy Xyratex Inc.

Email: sai_narasimhamurthy@xyratex.com Abstract**—The demand for parallel I/O performance continues**
to grow. However, modeling and generating parallel I/O workloads are challenging for several reasons including the large number of processes, I/O request dependencies and workload scalability. In this paper, we propose the PIONEER, a complete solution to Parallel I/O workload characterizatioN and gEnERation. The core of PIONEER is a proposed *generic workload path*,
which is essentially an abstract and dense representation of the parallel I/O patterns for all processes in a High Performance Computing (HPC) application. The generic workload path can be built via exploring the inter-processes correlations, I/O dependencies as well as file open session properties. We demonstrate the effectiveness of PIONEER by faithfully generating synthetic workloads for two popular HPC benchmarks and one real HPC application.

## I. Introduction

The computing scale is currently expanding from Petascale to Exascale. This will make the data parallelism even more challenging. Thoroughly understanding parallel I/O workloads is therefore critical for designing storage systems and improving their performance. Synthetic parallel I/O workload generation tools are also greatly needed in storage system performance tuning, testing and measurement.

Many HPC benchmarks such as IOR2 [1], NPB [2], and FLASH-IO [3] are developed to help test system performance.

HPC benchmarks are usually easy to use and can be tweaked by the users. However, real HPC applications in many scientific domains keep emerging such that system designers often have a hard time to find a benchmark which can represent a particular I/O workload for their need. Furthermore, many HPC benchmarks have little control on certain IO dimensions such as the IO arrival pattern [4], [5].

On the other hand, synthetic parallel I/O workload generation based on existing traces is more promising and practical as long as I/O tracing tools are enabled in a production environment. There is no need to access the source code of a real HPC application. Besides, the characteristics of the existing traces can be modified and tuned to generate a desired workload pattern.

However, parallel I/O workload modeling and synthesizing are very challenging. Raw traces captured by tools like *LANLTrace* framework tool [6] have to be sanitized before workload characterizing and modeling. Each I/O record contains several This work is partially supported by NSF Awards: 1439622, 1421913, 1305237 and 1217569. This work is also partially supported by Xyratex Inc.

important fields including timestamp, request type, request argument list, execution time, etc. A comprehensive workload modeling and generation framework should consider all of these important factors.

A parallel I/O trace usually contains both POSIX-IO operations and MPI-IO operations, which may have quite different syntaxes and arguments. For example, *SYS_open* requires 3 arguments but *MPI_File_iread* needs 5 arguments. However, disk I/O workloads usually only deal with two operation types
(READ and WRITE) with unified syntax and argument dimensions. Furthermore, there are all kinds of request dependencies in both the POSIX-IO library and the MPI-IO library. For example, *MPI_File_iread* depends on *MPI_File_open* since no process will be able to access the file data without opening it first. The challenges of modeling and generating synthetic parallel I/O workloads can be easily recognized when one realizes that actual parallel I/O workloads are generated by hundreds or thousands of these processes and these processes are correlated in specific ways.

Many prior studies such as [7]–[10] have been done on parallel I/O characterization to acquire meaningful characteristics to unveil application behaviors and provide valuable insights for parallel I/O workload synthesizing. However, as far as we know, there are rarely any complete parallel I/O
workload synthesizing solutions that generate realistic parallel I/O workloads. Most of the existing parallel I/O modeling and synthesizing studies focus on a single dimension such as inter-arrival time [4], [5] or request offset [11]. The existing two-dimensional [12] or multi-dimensional characterization schemes [13] for block I/O workloads cannot be applied to parallel I/O workload modeling directly because they do not consider the uniqueness of parallel I/O workloads.

Our solution in this paper handles these uniqueness and challenges with effective approaches. We propose the concept of a "*generic workload path*" based on the inter-process correlations to abstract and present the I/O patterns of all processes; we also set *library enforcement rules* to deal with the I/O library complexities and request dependencies; we develop the *file open session framework* to describe the file access patterns in the generic workload path; we characterize all the I/O operations that appeared in the generic workload path, the outcome of which can be tuned to generate a corresponding *synthetic generic workload path*; and we also develop a *workload generation engine* to expand a synthetic generic workload path into a complete parallel I/O workload, which can be scaled with any desired number of processes.

Details about these approaches will be presented in Section V. These approaches together enable us to take the initial step to propose a robust and scalable solution in this paper. As demonstrated later, our solution can significantly reduce the overhead of workload tracing, characterization and generation.

The structure of this paper is as follows. We introduce some background and related work in Section II. In Section III, we discuss the uniqueness of parallel I/O workloads and consequent challenges, followed by corresponding approaches in Section IV. We then propose our comprehensive solution of parallel I/O characterization and synthesizing in Section V. We demonstrate the effectiveness of our solution with experiments in Section VI. We finally make some conclusions in Section VII.

## Ii. Background And Related Work

In this section, we introduce the assumed HPC environment and the software applications that generate the parallel I/O
workloads. We will also summarize the existing work and show that major gaps exist in characterizing parallel I/O
workloads and in generating synthetic parallel I/O workloads.

## A. Parallel I/O Workloads

The concept of "parallelism" exists in many layers of the HPC software stack, such as application, I/O library, and file system. Therefore it is important to define parallel I/O
workload carefully.

1) Assumed HPC Environment: A typical HPC system usually consists of several major components including computing nodes, I/O nodes (I/O servers), metadata servers and backend storage infrastructures. A real system may have different implementations for each of these components. For example, Lustre systems implement the I/O nodes as Object Storage Servers (OSSs), which manage one or more Object Storage Targets (OSTs). Back-end storage infrastructures could be fulfilled by a certain number of Storage Area Networks (SANs),
each of which can be dedicated to a single OST or shared by multiple OSTs. Metadata servers in these systems maintain and manage a unified logical namespace for parallel file systems.

2) Parallel I/O Software Applications: HPC applications typically execute hundreds or thousands of processes. These applications can be either computational intensive, I/O intensive or both. The I/O behaviors and access patterns of these HPC applications depend, to a large degree, on the way they access files and the I/O libraries. Different I/O libraries can be used such as POSIX-IO, MPI-IO, HDF5, netCDF, etc. In the scope of this work, we define parallel applications as those utilizing MPI-IO libraries and higher level I/O libraries built on top of MPI-IO libraries such as HDF5. The processes of these parallel applications, however, are also technically allowed to access files with POSIX-IO library. When a parallel application runs, it will assign an *MPI process rank* to each process. The MPI process rank is essentially an internal process ID used by the parallel application to identify each process. The root process is assigned with rank 0.

In the HPC realm, there are generally three types of files:
root exclusive files, shared files, and private files. A shared file is defined as a file that is shared by all participating processes, while a *private file* means that every process has its own version of this file, usually with customized file name.

The naming convention for private files is a common prefix string plus the MPI process rank. A *root exclusive file* is only accessed by the root process.

## B. Related Work

I/O workload characterization is important for understanding the workload and improving system performance. Different mathematical models were proposed to describe the workload patterns, either temporally or spatially. For example, Poisson process have been widely used to describe I/O arrival patterns until several prior studies indicated that statistical properties including autocorrelations and self-similarity [14]
exist in these workloads due to their nature of burstiness
[15]–[17]. Since then, various self-similarity oriented models have been proposed to emulate the burstiness feature. For example, FARIMA [16] and FBM [18] models were proposed to describe network traffic workloads and later applied to disk I/O workloads. Models that utilize multiple ON/OFF models
[19] or a combination of ON/OFF model and Cox's model
[20] also have been used to characterize the burstiness in storage systems. An I/O workload model based on the Alphastable process was also proposed to generate synthetic disk I/O workloads and parallel I/O workloads [4].

Although each of the above models has its own effectiveness in modeling the workload burstiness and temporal properties such as self-similarity, most of them are limited to only one dimension such as inter-arrival time or request offset. However, parallel I/O workloads contain important information in other dimensions as well. Therefore, models that can incorporate multiple dimensions become more preferred for workload characterization and generation. Wang *et al.* proposed a model that can model the spatio-temporal correlation via an entropy plot of two-dimensional disk I/O request sequence [12]. Another work by Sriram and Kushagra utilizes the probabilistic state transition diagram [21] to describe disk I/O workloads in a comprehensive way where multiple dimensions including inter-arrival time, operation type, LBA offset, etc. are all considered to some degree. This work was then extended by Delimitrou in [13] to generate synthetic disk I/O workloads in data centers. However, both the two-dimensional and multidimensional characterization mechanisms are not suitable for parallel I/O workloads because they did not consider parallel I/O dependencies, let alone inter-process correlations.

On the other hand, there exist several studies on characterizing parallel I/O workloads. For example, Wang *et al*. used a series of empirical distributions to characterize parallel scientific applications in [7]. Distributions are presented independently though. Carns *et al*. developed the Darshan I/O characterization tool that can unveil some I/O behaviors of applications at extreme scale [8]. Carns *et al*. then outlined a methodology for continuous, and scalable I/O characterization that instruments Darshan and utilizes coarse-grained information from storage devices and file systems to help further interpret application level behaviors [9]. Cope *et al*. worked on the IOVIS project and proposed a portable I/O tracing system and visualization method to help analyze captured parallel I/O traces in an endto-end manner [10]. However, most of these existing studies did not address the parallel I/O generation problem.

As a result, in this paper we take the initial step of proposing a complete solution to parallel I/O characterization and synthesizing. We will demonstrate later in the paper that this solution is robust and scalable.

## Iii. Uniqueness Of Parallel I/O Workloads

In this section, we describe the uniqueness and associated challenges of modeling parallel I/O workload characterization and synthesizing.

## A. Inter-Process Correlations

HPC applications usually execute hundreds or thousands of processes, with one root process and a bunch of child processes. When characterizing the MPI-IO Test traces that are published by LANL and IOR2 traces that we captured on a cluster system at *Minnesota Supercomputing Institute* [22], we made some interesting observations on inter-process correlations. In general, the child MPI processes always mimic the root process. In other words, child processes share most of the I/O requests with the root process except those root exclusive I/O requests which are unique to the root process only. Table I shows the operational statistics of one root process and four child processes in a captured parallel I/O trace. This trace (identified as 32PE_N-N_448K) is a running instance of MPI-IO Test published by LANL. The table shows the statistics for selected major POSIX-IO and MPI-IO
operations in the trace. The operational statistics of the child processes are nearly identical with each other. There is also a great similarity between the root process and child processes, except that the root process issues more POSIX-IO operations. This is mainly because the root process has to manage the parallel I/O environment. Besides, HPC application developers tend to choose the root process to perform temporary data management. This similarity can also be validated by profiling the *operation reuse distance*, which is defined as the number of operations between two successive appearances of same operation, for root process and all child processes. There may exist some HPC applications that their child processes are to be divided into several subgroups. The processes in each subgroup perform similar operations.

Besides, HPC applications usually make performance gains by utilizing advanced I/O features in MPI-IO and higher level I/O libraries such as collective I/O [23], when processes are accessing shared files. Shared files therefore tend to show stronger inter-process correlations than private files. Behind these advanced I/O operations, inter-process communications,

| Operation       | child x1   | child x2   | child x3   | child x4   | root   |
|-----------------|------------|------------|------------|------------|--------|
| SYS_open        | 195        | 195        | 195        | 195        | 257    |
| SYS_close       | 82         | 79         | 76         | 77         | 126    |
| SYS_read        | 10737      | 10309      | 10522      | 10306      | 11907  |
| SYS_write       | 9905       | 9889       | 9891       | 9887       | 21995  |
| SYS_fstat64     | 56         | 56         | 56         | 56         | 101    |
| SYS_fcntl64     | 102        | 102        | 96         | 96         | 334    |
| SYS_statfs64    | 4          | 4          | 4          | 4          | 5      |
| MPI_File_seek   | 18724      | 18724      | 18724      | 18724      | 18724  |
| MPI_File_iwrite | 9362       | 9362       | 9362       | 9362       | 9362   |
| MPI_File_iread  | 9362       | 9362       | 9362       | 9362       | 9362   |
| MPIO_Wait       | 18724      | 18724      | 18724      | 18724      | 18724  |
| MPI_File_close  | 2          | 2          | 2          | 2          | 2      |
| MPI_Barrier     | 73         | 73         | 73         | 73         | 73     |
| MPI_Wtime       | 56188      | 56188      | 56188      | 56188      | 56188  |

data manipulation and synchronization are transparently managed by the I/O libraries.

## B. Complexities Of I/O Libraries

Due to the I/O software stack depth in the parallel environment, parallel I/O workloads generally involve more than one I/O library, which include, in most cases, both POSIXIO and MPI-IO [9]. Computational science applications also typically use higher level I/O libraries such as HDF5 and parallel netCDF. The operations in these higher level I/O
libraries will be eventually translated into MPI-IO and POSIXIO operations. The LANL-Trace framework tool [6], [24] can capture both POSIX-IO and MPI-IO commands.

Besides, POSIX-IO library and MPI-IO library have a rich set of I/O operations. They both contain metadata operations and file data operations. *Metadata operations* deal with file manipulations such as create, delete, open, close, file handler manipulation and file pointer manipulation. *File data operation*, on the other hand, do I/O transfer jobs for either read or write. Metadata operations are important in a parallel I/O
environment. As explained in [25], the MDS servers play a critical role in large scale I/O since they may potentially become the performance bottleneck. As a result, it is important to characterize metadata operation patterns and the way they are related to file data operations as part of parallel I/O
workload modeling.

Furthermore, unlike disk I/O operations, file system level I/O operations have to follow certain rules to be functional and meaningful. In this paper, we classify these rules into two categories: *hard dependency* and *soft dependency*. Hard dependency has to be respected when generating synthetic file system I/O workloads and thus is mandatory. A simplest example for hard dependency will be that a process has to open a file first before it can read/write that file. Soft dependency, on the other hand, is optional, yet important for creating meaningful and high quality synthetic workloads. For instance, a process may need to seek to a specific offset inside an opened file in order to access a particular data section. Both POSIX-IO and MPI-IO libraries have to consider these dependencies.

![3_image_0.png](3_image_0.png) 

## C. File Access Pattern

File open session is one of the most important aspects because there are many important properties associated with it, including sequentiality, *ownership* and *access mode*. Sequentiality describes the file offset pattern and a file session is said to show high sequentiality when offsets generally present monotonically increasing or decreasing patterns. In regards to file ownership, a file can be shared by multiple processes or dedicated to a single process during a given open session.

Access mode controls categories of file data operations that can be made to an opened file, such as read only, write only or both. Besides, there are some general characteristics of file open sessions, such as duration of file open sessions and frequencies of different I/O requests. Furthermore, metadata operation ratio and file data operation ratio are also important characterization criteria of file access patterns, considering the potential bottleneck at metadata servers. In conclusion, the access pattern of parallel I/O workloads can be summarized as the question: "which portion of which file is accessed by which process at what time in which way?" Our solution uses the framework of open session to answer this question and describe the parallel I/O patterns.

## Iv. Approaches To Uniqueness

In this subsection, we describe the proposed approaches to addressing the challenges of the identified uniqueness.

## A. Generic Workload Path

The large number of processes in an HPC application makes it impractical to characterize every single process separately although this is possible. Besides, even assuming that significant time and effort can be afforded for characterizing all the processes, challenges for scaling the workloads will be faced. Our proposed generic workload path provides a more efficient and better way to characterize parallel I/O workloads.

Simple HPC applications may require only two processes, one root process and one child process, to create a generic workload path. In this paper, we will consider this simple case.

However, the same technique can be applied if child processes are partitioned into multiple subgroups. In other words, the generic workload path constructed with a very small number of processes is a dense representation of all processes. We then assign a *global sequence ID* to each of the operations in the generic workload path. The global sequence ID is the index number of a request to indicate the request order and position in the generic workload path or trace file. For example, the first request has global sequence ID 0.

There are generally three advantages creating a generic workload path. First of all, by merging the root process and selected child processes into a single generic workload path, we can characterize the correlation and mix patterns between their I/O requests with the help of global sequence IDs.

Secondly, generic workload path makes our solution highly scalable and robust because it works like a workload template which will be customized for each process based on their MPI process ranks when executed by our workload generation engine. Any desired number of processes can be used during the execution so we can scale the workload by varying the number of processes. For example, we can synthesize a parallel I/O workload of 1000 processes based on an original workload of 100 processes for scaling up, or synthesize a workload of 50 processes for scaling down. The approach of extracting, characterizing, synthesizing and executing generic workload path will be presented later in Section V.

Another great benefit of using generic workload path is that we may now only need to capture I/O traces for a small number of processes instead of all the processes, which significantly reduces the capturing overhead and resulting trace sizes. Besides, unnecessary characterization on most child processes can also be avoided.

We use *slack time* as timing control mechanism for the generic workload path. Slack time is defined as the time between the completion of a previous request and the start of the next request. We compute the slack time based on
<timestamp> and <execution time>. The use of slack time enables a synthetic workload to be executed in storage systems with different performance without overloading them.

## B. I/O Library Enforcement

Unlike the block level I/O workloads, parallel I/O workload generation has to respect the inherent request dependencies of POSIX-IO library and MPI-IO library. As indicated previously, we consider two types of dependencies: hard dependency and soft dependency.

Figure 1a shows an enforcement diagram that represents these two types of request dependencies for selected POSIXIO operations. Column 1 contains independent operations that do not require opening a file and are metadata operations.

Column 2, on the other hand, contains dependent operations that operate on opened files, including both metadata operations and file data operations. The solid arrow means hard dependencies; the dashed arrow means soft dependencies.

Operations in grey boxes can be unnecessary to be used together with their counterparts. For example, read/write may or may not be preceded by a seek operation in practice. Each open session begins with an open operation and ends with a close operation. For files accessed by MPI-IO operations, we have constructed a similar enforcement diagram in Figure 1b to represent the dependencies among selected MPI-IO operations.

In implementation, hard dependency is generally guaranteed by the framework of file open session described in Section IV. Dependent operations in Column 2 can only exist in corresponding file open sessions while independent operations are not limited by this constraint. Soft dependency is preserved by replacing certain operations with newly defined operations. For example, we replace *seek* and *read* operations with two new operations: *read0* and read1. *read0* indicates a read operation not preceded by a *seek* operation while *read1* indicates a preceded *read*. This is especially useful for MPIIO library where more operation combinations are possible.

For instance, the non-blocking *MPI_File_iread* can be either preceded by *MPI_File_seek*, followed by *MPIO_Wait* or both. New operations such as *MPI_File_iread11* therefore can be defined to indicate these soft dependencies. This technique can also be used to preserve some hard dependencies such that no *munmap* should occur before a corresponding *mmap* operation. We characterize these new operations in workload characterization to capture the operation patterns.

## C. Framework Of File Open Sessions

The file access patterns of parallel I/O workloads are complicated due to the fact that parallel I/O workloads usually deal with multiple processes, multiple files and multiple I/O
libraries. In order to model such a workload, we apply file open session oriented characterization. A file open session describes the I/O request pattern during a specific period which starts with opening this file and ends with closing it. A file can have multiple open sessions throughout the workload.

We assign a global sequence ID to each request in the workload and represent them into a framework of file open sessions. Figure 2 shows such an example framework. There are 4 files in this example: A and B are private files; C
and D are shared files. Each rectangle represents an open session of its corresponding file, where each numbered circle inside rectangles is an I/O request accessing this file. These operations are from column 2 which require opening the file first. Column 1 operations, on the other hand, are independent and thus are not restricted to be inside file open sessions.

A number of workload characteristics will be extracted as explained in Section V. Creating such a framework of open sessions helps explore how the I/O requests to different files are correlated and how the file data operations and metadata operations are mingled.

![4_image_0.png](4_image_0.png) 

## V. Procedure Of A Complete Solution

In this section, we describe a complete solution for parallel I/O workload characterization and generation. All the proposed approaches described previously are included in this implementation.

The whole procedure of our implemented solution has been summarized as five phases in Figure 3, including sanitization phase, generic workload path extraction phase, characterization phase, *synthetic generic workload path generation phase* and *parallel I/O generation phase*. As the flowchart shows, we first sanitize and reformat the raw parallel I/O traces, produced by LANL-Trace framework or captured by other tools, for the root process and the selected child process or processes. We then extract generic workload path using these sanitized traces. A framework of open session oriented I/O characterization on this generic workload path produces a set of characteristics which are later used to create a synthetic generic workload path. We input this synthetic generic workload path to our workload generation engine to generate the actual parallel I/O workload for a desired number of processes. In the case that no original workload is provided, this procedure starts with the synthetic generic workload path generation phase but requires the user to specify the desired workload characteristics. Each of these steps is detailed in the following subsections. The output of a previous phase is the input of the next phase.

## A. Sanitization Phase

The raw traces produced by LANL-Trace framework need to be sanitized due to the following reasons. First, some POSIXIO operations are not issued directly by the application but instead are internally used by MPI-IO operations. However, due to the fact that LANL-Trace framework tool is using ltrace to capture POSIX-IO operations, it will just record any POSIX-IO operation it sees, irrespective of whether it is issued by the application or internally called by MPI-IO operations. Therefore, these internally used POSIX-IO operations need

![5_image_0.png](5_image_0.png)

to be masked by a script program. Fortunately, LANL-Trace framework tags those MPI-IO operations that internally call POSIX-IO operations with <unfinished> and <resumed> tag pairs. Table II presents trace examples before and after trace sanitization. Before sanitization, there are four POSIX-IO operations between the tag pair of *MPI_File_open*, which are actually internally called by *MPI_File_open* itself. Thus we convert it into a single MPI-IO operation record line.

Besides, a unified trace format is used to keep consistency which contains 7 dimensions: <timestamp>, <operation type>, <file name>, <offset>, <request size>, <execution time> and <extra>. We decode the file descriptors or handlers into the actual file names for ease of characterization. These 7 dimensions help us to characterize the workload later.

Only the traces of the root process and one child process need to be sanitized unless the processes of a sophisticated HPC application are divided into multiple subgroups, in which case we randomly choose one child process from each of the subgroups and sanitize their traces. The number of required subgroups can be either provided by the application owner or obtained by a bootstrap characterization. In bootstrap characterization, we take a small portion of the traces, such as first 10% of the traces, to compare the similarity between the child processes. Child processes are said to be in the same subgroup if their traces show similar operational statistics and operation reuse distance profile as we described in Section III.

The number of subgroups can be decided as a result.

## B. Generic Workload Path Extraction Phase

The major task here is to separate the root exclusive I/O
requests from mimicked I/O operations by comparing the root process trace with the traces of the selected child processes.

To achieve this, we first create a shadow trace file for each of these sanitized traces. A shadow trace file only contains the
<operation type> and <file name> dimensions of its original trace file. The rank suffix in the shared file names should be removed though.

Let us denote the root shadow trace file as R and denote the child shadow trace file as C. Then we compare R and C using native Linux utilities such as *diff* command which will output the differences as line indices. Other comparison tools can also be used here. These differences indicate root exclusive requests. The indices of these root exclusive requests in R map them back to the original trace records in the root process trace, which will be annotated with a root exclusive flag in the <extra> dimension. Other auxiliary annotations such as offset of reads and writes to shared files can also be added to facilitate later characterization. This annotated root process trace becomes the generic workload path.

The most important reason why we merge the I/O patterns of the root process and child processes into a generic workload path, as discussed previously, is that it helps model how the root I/O requests are correlated with those of child processes, as shown in the next phase. If we characterize them separately, this connection information will be lost.

## C. Characterization Phase

We characterize the extracted generic workload path to obtain the characteristics of the selected dimensions. The characteristics of the *root exclusive files*, the *shared files* and the *private files* will be stored separately to reduce the statistical interference among them.

Requests in the generic workload path are organized into a framework of file open sessions according to the global sequence IDs and the specific request information. Requests fall either inside or outside file open sessions according to the request dependencies.

For workload profiling, we made a list of open session oriented characteristics as well as their definitions in Table III,
which will be represented as empirical Probability Distribution Functions or Probability Functions. We use these empirical distributions for synthesizing although some dimensions such as slack time can be even fitted into existing models like Pareto distribution. Many characteristics are measured by the global sequence IDs in the generic workload path and therefore they can model the correlations between the root process and the child processes inherently. For example in Figure 2, there are 3 requests issued to file A during its first open session. The second request to file A (with global sequence ID 4) will not be issued until two requests (with global sequence IDs 2 and 3) are made to other files. This kind of I/O behaviors can be well represented by Inter Request Gap (IRG).

Similarly, Inter Session Gap (ISG) can control how many I/O requests should be issued to other files between two successive file open sessions of a particular file. Duration
(D) describes the number of I/O requests that are issued to a particular file during one open session. File Open Times
(OT) means the frequency of opening a target file. We also use file sequentiality (SEQ) and Access Mode (AM) to control the specific I/O pattern of a particular file open session. SEQ
describes the I/O randomness and AM controls the types of file data operations. Some other standard characteristics such as request size (SIZE) and request offset (OFF) are also used. We use two characteristics to count the I/O request type frequency with RT1 for the frequency of different I/O requests inside file open sessions and RT2 for those outside file open sessions. This allows us to more precisely synthesize the generic workload path.

Furthermore, the characteristics can describe the I/O patterns more precisely when used together. For example, char10:48:52.404754 MPI_File_open(92, 0x807a7f8, 34, 0x8078e38, 0xbf83b9e0 <unfinished ...> 10:48:52.405470 SYS_statfs64(0x807a7f8, 84, 0xbf83b788, 0xbf83b788, 0x81dff4) = 0 <0.008302>
Before 10:48:52.414801 SYS_umask(022) = 077 <0.000025>
10:48:52.414866 SYS_umask(077) = 022 <0.000017>
10:48:52.414936 SYS_open("/panfs/caddypan.lanl.gov/scratch1/nobody/tests/OUTPUT.1206553581.0", 32768, 0600) = 36 <0.000301>
10:48:52.416760 <... MPI_File_open resumed>)=0 <0.011931>
After 10:48:52.404754 MPI_File_open /panfs/caddypan.lanl.gov/scratch1/nobody/tests/OUTPUT.1206553581.0 NULL NULL 0.011931 MPI_COMM_SELF,34

![6_image_0.png](6_image_0.png) 

acteristics IRG together with D can not only describe how the requests to different files are mixed with each other, but also control the arrival rate of requests to a particular file. For instance, in Figure 2, file A and file B have the same open duration (i.e., 3 requests) but the requests come to A in a more compact manner and go to B at a slower pace. OT and ISG
together can not only control the opening frequency of a file, but also manage when to open this file.

## D. Synthetic Generic Workload Path Generation Phase

The synthetic generic workload path can be constructed in a hierarchical manner with all the input characteristics. The input characteristics can also be modified or tuned to generate desired workload patterns.

In general, we first create the framework of file open sessions. I/O requests residing in these open sessions will be assigned a global sequence ID, and then we sample based on I/O characteristics previously discussed and determine the specific I/O request types and their associated arguments according to the input characteristics. Finally, we handle the I/O requests outside the file open sessions, which are generally metadata operations belonging to column 1 in Figure 1.

In practice, we use Algorithm 1 to create the basic framework of file open sessions. It is important to sample the dimensions in the right order so that the I/O enforcement rules and the framework of file open sessions are reflected in the synthesized generic workload path. We first set the number of files in each file type which can be the same as that of original workload (OS) or specified/modified by user. Then for each file, we need to determine how many times it will be opened in the synthetic generic workload path according to OT distribution. For each open session, we then decide their properties including duration (D), sequentiality (SEQ), and access mode (AM) using corresponding distributions. The global sequence ID of file open request can be calculated by adding an ISG to the global sequence ID of previous file close request to the same file. Specially, the open request of the first open session of each file assumes its previous file close request has global sequence ID 0. Inside each file open session, we already know the number of requests (including file close request) residing there and their global sequence IDs can be easily calculated by adding an IRG to that of a previous request. Meanwhile, the operation type can be decided by access mode (AM) of the file open session and request type (RT1) distribution together. The associated offset (i.e., <offset>) can be decide by the OFF distribution and the sequentiality of the corresponding file open session together. The associated request size (i.e., <request size>) can be drawn from SIZE distribution. <extra> dimension will be fulfilled with corresponding values or default values when applicable.

<slack time> can also be sampled based on a user specified distribution or the distribution in the original workload.

for *each file class* do

![6_image_2.png](6_image_2.png)

determine number of files (OS); for *each file* do determine number of open sessions (OT); for *each open session* do determine open session properties (D, SEQ, AM); compute file open request sequence ID (ISG); for *each operation* do compute its request sequence ID (IRG);
determine its operation type (RT1,AM);
determine its argument list (OFF, SEQ, SIZE);
end compute file close request sequence ID;

![6_image_1.png](6_image_1.png)

end Algorithm 1: Creating the framework of file open sessions After we complete the framework of file open sessions, we merge the requests to different files into a single request stream according to their global sequence IDs. Throughout

| TABLE III: I/O Characteristics   |                                                                                                                                                                                                                                                                                                                                                                         |
|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Terms                            | Explanation                                                                                                                                                                                                                                                                                                                                                             |
| Inter file session gap (ISG)     | The distance between the close of previous open session and the open of next open session of the same file, which is measured by the global sequence ID difference.                                                                                                                                                                                                     |
| Inter request gap (IRG)          | The distance between the previous request and the next request in the same file open session. It is also measured by the global sequence ID difference.                                                                                                                                                                                                                 |
| File open duration (D)           | The duration of a file open session, which is measured by the number of requests belonging to this open session.                                                                                                                                                                                                                                                        |
| File open times (OT)             | The number of open sessions of a file throughout the workload.                                                                                                                                                                                                                                                                                                          |
| File sizes (FS)                  | The file size, which is set to the maximum sum of request offset and request size in the workload.                                                                                                                                                                                                                                                                      |
| I/O request type (RT1)           | The frequency of different I/O requests inside file open sessions                                                                                                                                                                                                                                                                                                       |
| I/O request type (RT2)           | The frequency of different I/O requests outside file open sessions                                                                                                                                                                                                                                                                                                      |
| Offset (OFF)                     | The I/O access offset inside files                                                                                                                                                                                                                                                                                                                                      |
| Request size (SIZE)              | Simply the request size                                                                                                                                                                                                                                                                                                                                                 |
| File sequentiality (SEQ)         | The sequentiality of a file open session, measured by the ratio of sequential file I/O requests.                                                                                                                                                                                                                                                                        |
| File access mode (AM)            | The access mode of a file open session. The same file can be opened multiple times, each of which can have a different mode. For example, a new file can be created with write only mode and reopened later with read only mode. Access mode is important to synthetic workload generation because it constrains the request types that are allowed on the target file. |
| Ownership (OS)                   | The ownership of a file, which can be either root exclusive file, private file or shared file. We count number of files in each of the three file type.                                                                                                                                                                                                                 |
| Slack time (ST)                  | Slack time, defined as the time between the completion of the previous request and the start of the next request. It can follow different distributions such as the Pareto distribution.                                                                                                                                                                                |

the whole process, we make sure every global sequence ID is unique. This single request stream might end up with "holes",
which indicate the missing global sequence IDs. For example in Figure 4, the merged request stream is [1, 2, 3, 6, 7, 8, 12, 13, 14, 15, ...] with [4, 5], [9, 10, 11] missing. Requests in these holes are designated to be from column 1 in Figure 1 and they are metadata operations. This also shows how PIONEER
mingles the column 1 operations with other operations.

Now we need to sample the metadata operations that are outside the file open sessions. The operation type can be sampled according to RT2. If this metadata operation requires a filename, we first sample a file type according to the sizes of RT2 of the three file types. We then randomly choose a file in the selected file type. Default values can be used for the rest of the arguments. We can make this decision because these are metadata operations that do not require file descriptors and simply retrieve file related information.

The synthetic generic workload path is created at this point and ready to be executed with the wokload generation engine.

## E. Parallel I/O Generation Phase

We develop and implement our own workload generation engine to actually schedule and issue the requests in a real parallel I/O environment. The workload generation engine is essentially an MPI program written in C. It takes a synthetic generic workload path and the desired number of processes as input and generates corresponding parallel I/O workloads.

The principle of our execution engine is to convert the generic workload path into an MPI program whose computing job is emulated by the slack time between successive I/O requests and whose I/O job is represented by I/O requests in the synthetic generic workload path.

The child processes spawned by the execution engine customize the generic workload path based on their MPI process ranks and then issue I/O requests according to the patterns in the synthetic generic workload path. Since file descriptors will be reused in execution, we use actual file names as argument for those I/O requests requiring a file descriptor when generating the synthetic generic workload path in the previous phase. As a result, the workload generation engine can keep track of the file descriptor usage at runtime and translate the file names into the right file descriptors or handlers for both MPI-IO and POSIX-IO operations.

## Vi. Evaluation

The goal of our evaluations is to demonstrate the effectiveness of our solution by comparing original workloads and synthetic workloads using popular HPC benchmarks and applications, in a representative HPC environment.

## A. Target Applications And Traces

In our experiments, we use two popularly used HPC
benchmarks: MPI-IO Test and IOR2, both of which are I/Ointensive. We also use a real HPC application called iPic3D
[26]. MPI-IO Test is developed by LANL and is written with parallel I/O and scale in mind. As a result, it is popularly used to test parallel I/O performance at the scale of big clusters. By default, MPI-IO Test will write a specific pattern to a file, close the file, open the file for read, read the data, check for data integrity and close the file. Access patterns can be tuned with several parameters. On the other hand, IOR2 is part of the ASCI Purple Benchmarks developed at LLNL
for evaluating parallel I/O performance [27]. iPic3D, a threedimensional parallel code, is a high performance simulator of space weather. It uses HDF5 to store and access data.

We run the three applications using 1024 processes on Itasca
[28], a cluster system at Minnesota Supercomputing Institute. We capture their traces with LANL-Trace framework tool. LANL-Trace is also used when we synthesize corresponding parallel I/O workloads with our workload generation engine using the same numbers of processes. Itasca consists of more than 8000 compute cores and 24 TB of main memory. It also has a large Lustre file system storage (/lustre) of size more than 500 TB, which serves as a large shared scratch space.

![8_image_0.png](8_image_0.png)

![8_image_1.png](8_image_1.png) ![8_image_2.png](8_image_2.png)

In all of our experiments, principal data files used by both benchmarks reside on the Lustre file system.

## B. Comparison Metrics

For meaningful comparisons and evaluations, we use I/O
throughput and file data operation ratio, which are orthogonal to input parameters but implicitly controlled by our solution. We also use the request arrival rate to show the burstiness in the workload are also emulated.

1) I/O Throughput: I/O throughput shows how fast data can be read or written which inherently indicates the impact of slack time and request response time. We present in Figure 5 the comparison between the original workloads and the synthetic workloads in terms of I/O throughput in MB/ms.

The general patterns show that synthetic workloads match the original workloads well.

2) File Data Operation Ratio: Metadata operation overhead can have significant impact on I/O performance due to possible bottleneck at the metadata servers [25] so we choose file data operation ratio (1 - metadata operation ratio) as an evaluation metric. We present a comparison of file data operation ratio along time in Figure 6, after executing the generic workload paths. Each plot includes two parts, the file data operation ratio (red and blue lines, measured by left Y axis) and the absolute number of file data operations (green lines, measured by right Y axis). We can observe that the green lines generally follow the I/O throughput curves, meaning that I/O throughput is dominated by the number of read/write requests and the read/write request sizes have small variations, which is true after inspecting the traces. As a result, we believe the synthetic workload emulate the originals very well.

3) Arrival Rate: Figure 7 shows the arrival rate comparisons. X axis is the number of operations per time unit. A
larger X value means I/O is more bursty in that time period.

Y axis is the corresponding cumulative distribution function.

MPI-IO Test has a more stable I/O patterns than IOR2 and iPic3D, as seen in I/O throughput and file data operation ratio plottings. As a result, its arrival rate is relatively easier to preserve in the synthetic workload. For IOR2 and iPic3D, the original workload and synthetic workload are close to each other especially when the numbers of operations per time unit are large, meaning that the burstiness is well modeled.

## Vii. Conclusions

In this paper, we propose a complete solution to parallel I/O
workload characterization and synthesizing. Unlike existing work, we deal with several uniqueness and challenges with parallel I/O workloads, including inter-process correlations, I/O library complexities and dependencies, as well as specific file access patterns.

In our solution, we first shrink original parallel I/O workloads into a generic workload path by exploiting and utilizing the inter-process correlations. Then we characterize and model the resulting generic workload path, where we set enforcement rules to preserve I/O request dependencies and we model file access patterns by profiling file open sessions. We use the extracted characteristics to construct a synthetic generic workload path. Our complete solution also includes a workload generation engine that can expand the synthetic generic workload path into a complete parallel I/O workload for a desired number of processes. Our solution is demonstrated to be effective via experimenting with two popular HPC benchmarks and a real HPC application.

## References

[8] P. Carns, R. Latham, R. Ross, K. Iskra, S. Lang, and K. Riley, "24/7 characterization of petascale i/o workloads," in *Cluster Computing and* Workshops, 2009. CLUSTER'09. IEEE International Conference on. IEEE, 2009, pp. 1–10.

[9] P. Carns, K. Harms, W. Allcock, C. Bacon, S. Lang, R. Latham, and R. Ross, "Understanding and improving computational science storage access through continuous characterization," ACM Transactions on Storage (TOS), vol. 7, no. 3, p. 8, 2011.

[10] C. Muelder, C. Sigovan, K.-L. Ma, J. Cope, S. Lang, K. Iskra, P. Beckman, and R. Ross, "Visual analysis of i/o system behavior for highend computing," in Proceedings of the third international workshop on Large-scale system and application performance. ACM, 2011, pp. 19–
26.

[11] J. Oly and D. A. Reed, "Markov model prediction of i/o requests for scientific applications," in *Proceedings of the 16th international* conference on Supercomputing. ACM, 2002, pp. 147–155.

[12] M. Wang, A. Ailamaki, and C. Faloutsos, "Capturing the spatio-temporal behavior of real traffic data," *Performance Evaluation*, vol. 49, no. 1, pp. 147–163, 2002.

[13] C. Delimitrou, S. Sankar, K. Vaid, and C. Kozyrakis, "Accurate modeling and generation of storage i/o for datacenter workloads," in *Proceedings* of the 2nd Workshop on Exascale Evaluation and Research Techniques, EXERT, Newport Beach, CA (March 2011), 2011.

[14] G. Horn, A. Kvalbein, J. Blomskøld, and E. Nilsen, "An empirical comparison of generators for self similar simulated traffic," Performance Evaluation, vol. 64, no. 2, pp. 162–190, 2007.

[15] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson, "On the self-similar nature of ethernet traffic (extended version)," *Networking,* IEEE/ACM Transactions on, vol. 2, no. 1, pp. 1–15, 1994.

[16] M. E. Crovella and A. Bestavros, "Self-similarity in world wide web traffic: evidence and possible causes," *Networking, IEEE/ACM Transactions on*, vol. 5, no. 6, pp. 835–846, 1997.

[17] P. A. Barreto, P. H. de Carvalho, J. A. Soares, and H. Abdalla Jr, "A traffic characterization procedure for multimedia applications in converged networks," in Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, 2005. 13th IEEE International Symposium on. IEEE, 2005, pp. 153–160.

[18] C. Stathis and B. Maglaris, "Modelling the self-similar behaviour of network traffic," *Computer Networks*, vol. 34, no. 1, pp. 37–47, 2000.

[19] M. E. Gomez and V. Santonja, "Analysis of self-similarity in i/o workload using structural modeling," in *Modeling, Analysis and Simulation* of Computer and Telecommunication Systems, 1999. Proceedings. 7th International Symposium on. IEEE, 1999, pp. 234–242.

[20] V. Paxson and S. Floyd, "Wide area traffic: the failure of poisson modeling," *IEEE/ACM Transactions on Networking (ToN)*, vol. 3, no. 3, pp. 226–244, 1995.

[21] S. Sankar and K. Vaid, "Storage characterization for unstructured data in online services applications," in *Workload Characterization, 2009.* IISWC 2009. IEEE International Symposium on. IEEE, 2009, pp. 148– 157.

[22] "Minnesota Supercomputing Institute." https://www.msi.umn.edu/.

[23] R. Thakur, W. Gropp, and E. Lusk, "Data sieving and collective i/o in romio," in *Frontiers of Massively Parallel Computation, 1999. Frontiers'* 99. *The Seventh Symposium on the*. IEEE, 1999, pp. 182–189.

[24] A. Konwinski, J. Bent, J. Nunez, and M. Quist, "Towards an i/o tracing framework taxonomy," in *Proceedings of the 2nd international workshop* on Petascale data storage: held in conjunction with Supercomputing'07. ACM, 2007, pp. 56–62.

[25] S. V. Patil, G. A. Gibson, S. Lang, and M. Polte, "Giga+: scalable directories for shared file systems," in Proceedings of the 2nd international workshop on Petascale data storage: held in conjunction with Supercomputing'07. ACM, 2007, pp. 26–29.

[26] "iPic3D." https://github.com/CmPA/iPic3D. [27] H. Shan and J. Shalf, "Using ior to analyze the i/o performance for hpc platforms," 2007.

[28] "Itasca." https://www.msi.umn.edu/hpc/itasca.